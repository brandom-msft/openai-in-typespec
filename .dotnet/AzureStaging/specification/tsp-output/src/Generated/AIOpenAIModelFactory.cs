// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.

// <auto-generated/>

#nullable disable

using System;
using System.Collections.Generic;
using System.Linq;

namespace Azure.AI.OpenAI
{
    /// <summary> Model factory for models. </summary>
    public static partial class AIOpenAIModelFactory
    {
        /// <summary> Initializes a new instance of <see cref="OpenAI.CreateChatCompletionRequest"/>. </summary>
        /// <param name="messages"> A list of messages comprising the conversation so far. [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models). </param>
        /// <param name="model"> ID of the model to use. See the [model endpoint compatibility](/docs/models/model-endpoint-compatibility) table for details on which models work with the Chat API. </param>
        /// <param name="frequencyPenalty">
        /// Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
        ///
        /// [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)
        /// </param>
        /// <param name="logitBias">
        /// Modify the likelihood of specified tokens appearing in the completion.
        ///
        /// Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
        /// </param>
        /// <param name="logprobs"> Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`. </param>
        /// <param name="topLogprobs"> An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used. </param>
        /// <param name="maxTokens">
        /// The maximum number of [tokens](/tokenizer) that can be generated in the chat completion.
        ///
        /// The total length of input tokens and generated tokens is limited by the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens.
        /// </param>
        /// <param name="n"> How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs. </param>
        /// <param name="presencePenalty">
        /// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
        ///
        /// [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)
        /// </param>
        /// <param name="responseFormat">
        /// An object specifying the format that the model must output. Compatible with [GPT-4 Turbo](/docs/models/gpt-4-and-gpt-4-turbo) and all GPT-3.5 Turbo models newer than `gpt-3.5-turbo-1106`.
        ///
        /// Setting to `{ "type": "json_object" }` enables JSON mode, which guarantees the message the model generates is valid JSON.
        ///
        /// **Important:** when using JSON mode, you **must** also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly "stuck" request. Also note that the message content may be partially cut off if `finish_reason="length"`, which indicates the generation exceeded `max_tokens` or the conversation exceeded the max context length.
        /// </param>
        /// <param name="seed">
        /// This feature is in Beta.
        /// If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.
        /// Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.
        /// </param>
        /// <param name="stop"> Up to 4 sequences where the API will stop generating further tokens. </param>
        /// <param name="stream"> If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions). </param>
        /// <param name="streamOptions"></param>
        /// <param name="temperature">
        /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
        ///
        /// We generally recommend altering this or `top_p` but not both.
        /// </param>
        /// <param name="topP">
        /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
        ///
        /// We generally recommend altering this or `temperature` but not both.
        /// </param>
        /// <param name="tools"> A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported. </param>
        /// <param name="toolChoice"></param>
        /// <param name="user"> A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids). </param>
        /// <param name="functionCall">
        /// Deprecated in favor of `tool_choice`.
        ///
        /// Controls which (if any) function is called by the model.
        /// `none` means the model will not call a function and instead generates a message.
        /// `auto` means the model can pick between generating a message or calling a function.
        /// Specifying a particular function via `{"name": "my_function"}` forces the model to call that function.
        ///
        /// `none` is the default when no functions are present. `auto` is the default if functions are present.
        /// </param>
        /// <param name="functions">
        /// Deprecated in favor of `tools`.
        ///
        /// A list of functions the model may generate JSON inputs for.
        /// </param>
        /// <returns> A new <see cref="OpenAI.CreateChatCompletionRequest"/> instance for mocking. </returns>
        public static CreateChatCompletionRequest CreateChatCompletionRequest(IEnumerable<BinaryData> messages = null, string model = null, float? frequencyPenalty = null, IReadOnlyDictionary<string, int> logitBias = null, bool? logprobs = null, int? topLogprobs = null, int? maxTokens = null, int? n = null, float? presencePenalty = null, CreateChatCompletionRequestResponseFormat responseFormat = null, long? seed = null, BinaryData stop = null, bool? stream = null, ChatCompletionStreamOptions streamOptions = null, float? temperature = null, float? topP = null, IEnumerable<ChatCompletionTool> tools = null, BinaryData toolChoice = null, string user = null, BinaryData functionCall = null, IEnumerable<ChatCompletionFunctions> functions = null)
        {
            messages ??= new List<BinaryData>();
            logitBias ??= new Dictionary<string, int>();
            tools ??= new List<ChatCompletionTool>();
            functions ??= new List<ChatCompletionFunctions>();

            return new CreateChatCompletionRequest(
                messages?.ToList(),
                model,
                frequencyPenalty,
                logitBias,
                logprobs,
                topLogprobs,
                maxTokens,
                n,
                presencePenalty,
                responseFormat,
                seed,
                stop,
                stream,
                streamOptions,
                temperature,
                topP,
                tools?.ToList(),
                toolChoice,
                user,
                functionCall,
                functions?.ToList(),
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionRequestSystemMessage"/>. </summary>
        /// <param name="content"> The contents of the system message. </param>
        /// <param name="role"> The role of the messages author, in this case `system`. </param>
        /// <param name="name"> An optional name for the participant. Provides the model information to differentiate between participants of the same role. </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionRequestSystemMessage"/> instance for mocking. </returns>
        public static ChatCompletionRequestSystemMessage ChatCompletionRequestSystemMessage(string content = null, string role = null, string name = null)
        {
            return new ChatCompletionRequestSystemMessage(content, role, name, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionRequestUserMessage"/>. </summary>
        /// <param name="content"> The contents of the user message. </param>
        /// <param name="role"> The role of the messages author, in this case `user`. </param>
        /// <param name="name"> An optional name for the participant. Provides the model information to differentiate between participants of the same role. </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionRequestUserMessage"/> instance for mocking. </returns>
        public static ChatCompletionRequestUserMessage ChatCompletionRequestUserMessage(BinaryData content = null, string role = null, string name = null)
        {
            return new ChatCompletionRequestUserMessage(content, role, name, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionRequestMessageContentPartText"/>. </summary>
        /// <param name="type"> The type of the content part. </param>
        /// <param name="text"> The text content. </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionRequestMessageContentPartText"/> instance for mocking. </returns>
        public static ChatCompletionRequestMessageContentPartText ChatCompletionRequestMessageContentPartText(string type = null, string text = null)
        {
            return new ChatCompletionRequestMessageContentPartText(type, text, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionRequestMessageContentPartImage"/>. </summary>
        /// <param name="type"> The type of the content part. </param>
        /// <param name="imageUrl"></param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionRequestMessageContentPartImage"/> instance for mocking. </returns>
        public static ChatCompletionRequestMessageContentPartImage ChatCompletionRequestMessageContentPartImage(string type = null, ChatCompletionRequestMessageContentPartImageImageUrl imageUrl = null)
        {
            return new ChatCompletionRequestMessageContentPartImage(type, imageUrl, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionRequestMessageContentPartImageImageUrl"/>. </summary>
        /// <param name="url"> Either a URL of the image or the base64 encoded image data. </param>
        /// <param name="detail"> Specifies the detail level of the image. Learn more in the [Vision guide](/docs/guides/vision/low-or-high-fidelity-image-understanding). </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionRequestMessageContentPartImageImageUrl"/> instance for mocking. </returns>
        public static ChatCompletionRequestMessageContentPartImageImageUrl ChatCompletionRequestMessageContentPartImageImageUrl(Uri url = null, string detail = null)
        {
            return new ChatCompletionRequestMessageContentPartImageImageUrl(url, detail, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionRequestAssistantMessage"/>. </summary>
        /// <param name="content"> The contents of the assistant message. Required unless `tool_calls` or `function_call` is specified. </param>
        /// <param name="role"> The role of the messages author, in this case `assistant`. </param>
        /// <param name="name"> An optional name for the participant. Provides the model information to differentiate between participants of the same role. </param>
        /// <param name="toolCalls"></param>
        /// <param name="functionCall"> Deprecated and replaced by `tool_calls`. The name and arguments of a function that should be called, as generated by the model. </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionRequestAssistantMessage"/> instance for mocking. </returns>
        public static ChatCompletionRequestAssistantMessage ChatCompletionRequestAssistantMessage(string content = null, string role = null, string name = null, IEnumerable<ChatCompletionMessageToolCall> toolCalls = null, ChatCompletionRequestAssistantMessageFunctionCall functionCall = null)
        {
            toolCalls ??= new List<ChatCompletionMessageToolCall>();

            return new ChatCompletionRequestAssistantMessage(
                content,
                role,
                name,
                toolCalls?.ToList(),
                functionCall,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionMessageToolCall"/>. </summary>
        /// <param name="id"> The ID of the tool call. </param>
        /// <param name="type"> The type of the tool. Currently, only `function` is supported. </param>
        /// <param name="function"> The function that the model called. </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionMessageToolCall"/> instance for mocking. </returns>
        public static ChatCompletionMessageToolCall ChatCompletionMessageToolCall(string id = null, string type = null, ChatCompletionMessageToolCallFunction function = null)
        {
            return new ChatCompletionMessageToolCall(id, type, function, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionMessageToolCallFunction"/>. </summary>
        /// <param name="name"> The name of the function to call. </param>
        /// <param name="arguments"> The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function. </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionMessageToolCallFunction"/> instance for mocking. </returns>
        public static ChatCompletionMessageToolCallFunction ChatCompletionMessageToolCallFunction(string name = null, string arguments = null)
        {
            return new ChatCompletionMessageToolCallFunction(name, arguments, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionRequestAssistantMessageFunctionCall"/>. </summary>
        /// <param name="arguments"> The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function. </param>
        /// <param name="name"> The name of the function to call. </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionRequestAssistantMessageFunctionCall"/> instance for mocking. </returns>
        public static ChatCompletionRequestAssistantMessageFunctionCall ChatCompletionRequestAssistantMessageFunctionCall(string arguments = null, string name = null)
        {
            return new ChatCompletionRequestAssistantMessageFunctionCall(arguments, name, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionRequestToolMessage"/>. </summary>
        /// <param name="role"> The role of the messages author, in this case `tool`. </param>
        /// <param name="content"> The contents of the tool message. </param>
        /// <param name="toolCallId"> Tool call that this message is responding to. </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionRequestToolMessage"/> instance for mocking. </returns>
        public static ChatCompletionRequestToolMessage ChatCompletionRequestToolMessage(string role = null, string content = null, string toolCallId = null)
        {
            return new ChatCompletionRequestToolMessage(role, content, toolCallId, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionRequestFunctionMessage"/>. </summary>
        /// <param name="role"> The role of the messages author, in this case `function`. </param>
        /// <param name="content"> The contents of the function message. </param>
        /// <param name="name"> The name of the function to call. </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionRequestFunctionMessage"/> instance for mocking. </returns>
        public static ChatCompletionRequestFunctionMessage ChatCompletionRequestFunctionMessage(string role = null, string content = null, string name = null)
        {
            return new ChatCompletionRequestFunctionMessage(role, content, name, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.CreateChatCompletionRequestResponseFormat"/>. </summary>
        /// <param name="type"> Must be one of `text` or `json_object`. </param>
        /// <returns> A new <see cref="OpenAI.CreateChatCompletionRequestResponseFormat"/> instance for mocking. </returns>
        public static CreateChatCompletionRequestResponseFormat CreateChatCompletionRequestResponseFormat(string type = null)
        {
            return new CreateChatCompletionRequestResponseFormat(type, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionStreamOptions"/>. </summary>
        /// <param name="includeUsage"> If set, an additional chunk will be streamed before the `data: [DONE]` message. The `usage` field on this chunk shows the token usage statistics for the entire request, and the `choices` field will always be an empty array. All other chunks will also include a `usage` field, but with a null value. </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionStreamOptions"/> instance for mocking. </returns>
        public static ChatCompletionStreamOptions ChatCompletionStreamOptions(bool? includeUsage = null)
        {
            return new ChatCompletionStreamOptions(includeUsage, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionTool"/>. </summary>
        /// <param name="type"> The type of the tool. Currently, only `function` is supported. </param>
        /// <param name="function"></param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionTool"/> instance for mocking. </returns>
        public static ChatCompletionTool ChatCompletionTool(string type = null, FunctionObject function = null)
        {
            return new ChatCompletionTool(type, function, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.FunctionObject"/>. </summary>
        /// <param name="description"> A description of what the function does, used by the model to choose when and how to call the function. </param>
        /// <param name="name"> The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64. </param>
        /// <param name="parameters"></param>
        /// <returns> A new <see cref="OpenAI.FunctionObject"/> instance for mocking. </returns>
        public static FunctionObject FunctionObject(string description = null, string name = null, FunctionParameters parameters = null)
        {
            return new FunctionObject(description, name, parameters, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.FunctionParameters"/>. </summary>
        /// <param name="additionalProperties"> Additional Properties. </param>
        /// <returns> A new <see cref="OpenAI.FunctionParameters"/> instance for mocking. </returns>
        public static FunctionParameters FunctionParameters(IReadOnlyDictionary<string, BinaryData> additionalProperties = null)
        {
            additionalProperties ??= new Dictionary<string, BinaryData>();

            return new FunctionParameters(additionalProperties);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionNamedToolChoice"/>. </summary>
        /// <param name="type"> The type of the tool. Currently, only `function` is supported. </param>
        /// <param name="function"></param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionNamedToolChoice"/> instance for mocking. </returns>
        public static ChatCompletionNamedToolChoice ChatCompletionNamedToolChoice(string type = null, ChatCompletionNamedToolChoiceFunction function = null)
        {
            return new ChatCompletionNamedToolChoice(type, function, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionNamedToolChoiceFunction"/>. </summary>
        /// <param name="name"> The name of the function to call. </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionNamedToolChoiceFunction"/> instance for mocking. </returns>
        public static ChatCompletionNamedToolChoiceFunction ChatCompletionNamedToolChoiceFunction(string name = null)
        {
            return new ChatCompletionNamedToolChoiceFunction(name, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionFunctionCallOption"/>. </summary>
        /// <param name="name"> The name of the function to call. </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionFunctionCallOption"/> instance for mocking. </returns>
        public static ChatCompletionFunctionCallOption ChatCompletionFunctionCallOption(string name = null)
        {
            return new ChatCompletionFunctionCallOption(name, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionFunctions"/>. </summary>
        /// <param name="description"> A description of what the function does, used by the model to choose when and how to call the function. </param>
        /// <param name="name"> The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64. </param>
        /// <param name="parameters"></param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionFunctions"/> instance for mocking. </returns>
        public static ChatCompletionFunctions ChatCompletionFunctions(string description = null, string name = null, FunctionParameters parameters = null)
        {
            return new ChatCompletionFunctions(description, name, parameters, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureCreateChatCompletionRequest"/>. </summary>
        /// <param name="messages"> A list of messages comprising the conversation so far. [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models). </param>
        /// <param name="model"> ID of the model to use. See the [model endpoint compatibility](/docs/models/model-endpoint-compatibility) table for details on which models work with the Chat API. </param>
        /// <param name="frequencyPenalty">
        /// Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
        ///
        /// [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)
        /// </param>
        /// <param name="logitBias">
        /// Modify the likelihood of specified tokens appearing in the completion.
        ///
        /// Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
        /// </param>
        /// <param name="logprobs"> Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`. </param>
        /// <param name="topLogprobs"> An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used. </param>
        /// <param name="maxTokens">
        /// The maximum number of [tokens](/tokenizer) that can be generated in the chat completion.
        ///
        /// The total length of input tokens and generated tokens is limited by the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens.
        /// </param>
        /// <param name="n"> How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs. </param>
        /// <param name="presencePenalty">
        /// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
        ///
        /// [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)
        /// </param>
        /// <param name="responseFormat">
        /// An object specifying the format that the model must output. Compatible with [GPT-4 Turbo](/docs/models/gpt-4-and-gpt-4-turbo) and all GPT-3.5 Turbo models newer than `gpt-3.5-turbo-1106`.
        ///
        /// Setting to `{ "type": "json_object" }` enables JSON mode, which guarantees the message the model generates is valid JSON.
        ///
        /// **Important:** when using JSON mode, you **must** also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly "stuck" request. Also note that the message content may be partially cut off if `finish_reason="length"`, which indicates the generation exceeded `max_tokens` or the conversation exceeded the max context length.
        /// </param>
        /// <param name="seed">
        /// This feature is in Beta.
        /// If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.
        /// Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.
        /// </param>
        /// <param name="stop"> Up to 4 sequences where the API will stop generating further tokens. </param>
        /// <param name="stream"> If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions). </param>
        /// <param name="streamOptions"></param>
        /// <param name="temperature">
        /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
        ///
        /// We generally recommend altering this or `top_p` but not both.
        /// </param>
        /// <param name="topP">
        /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
        ///
        /// We generally recommend altering this or `temperature` but not both.
        /// </param>
        /// <param name="tools"> A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported. </param>
        /// <param name="toolChoice"></param>
        /// <param name="user"> A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids). </param>
        /// <param name="functionCall">
        /// Deprecated in favor of `tool_choice`.
        ///
        /// Controls which (if any) function is called by the model.
        /// `none` means the model will not call a function and instead generates a message.
        /// `auto` means the model can pick between generating a message or calling a function.
        /// Specifying a particular function via `{"name": "my_function"}` forces the model to call that function.
        ///
        /// `none` is the default when no functions are present. `auto` is the default if functions are present.
        /// </param>
        /// <param name="functions">
        /// Deprecated in favor of `tools`.
        ///
        /// A list of functions the model may generate JSON inputs for.
        /// </param>
        /// <param name="dataSources">
        /// Please note <see cref="AzureChatDataSource"/> is the base class. According to the scenario, a derived class of the base class might need to be assigned here, or this property needs to be casted to one of the possible derived classes.
        /// The available derived classes include <see cref="OpenAI.AzureChatMachineLearningIndexDataSource"/>, <see cref="OpenAI.AzureChatSearchDataSource"/>, <see cref="OpenAI.AzureChatCosmosDBDataSource"/>, <see cref="OpenAI.AzureChatElasticsearchDataSource"/> and <see cref="OpenAI.AzureChatPineconeDataSource"/>.
        /// </param>
        /// <returns> A new <see cref="OpenAI.AzureCreateChatCompletionRequest"/> instance for mocking. </returns>
        public static AzureCreateChatCompletionRequest AzureCreateChatCompletionRequest(IEnumerable<BinaryData> messages = null, string model = null, float? frequencyPenalty = null, IReadOnlyDictionary<string, int> logitBias = null, bool? logprobs = null, int? topLogprobs = null, int? maxTokens = null, int? n = null, float? presencePenalty = null, CreateChatCompletionRequestResponseFormat responseFormat = null, long? seed = null, BinaryData stop = null, bool? stream = null, ChatCompletionStreamOptions streamOptions = null, float? temperature = null, float? topP = null, IEnumerable<ChatCompletionTool> tools = null, BinaryData toolChoice = null, string user = null, BinaryData functionCall = null, IEnumerable<ChatCompletionFunctions> functions = null, IEnumerable<AzureChatDataSource> dataSources = null)
        {
            messages ??= new List<BinaryData>();
            logitBias ??= new Dictionary<string, int>();
            tools ??= new List<ChatCompletionTool>();
            functions ??= new List<ChatCompletionFunctions>();
            dataSources ??= new List<AzureChatDataSource>();

            return new AzureCreateChatCompletionRequest(
                messages?.ToList(),
                model,
                frequencyPenalty,
                logitBias,
                logprobs,
                topLogprobs,
                maxTokens,
                n,
                presencePenalty,
                responseFormat,
                seed,
                stop,
                stream,
                streamOptions,
                temperature,
                topP,
                tools?.ToList(),
                toolChoice,
                user,
                functionCall,
                functions?.ToList(),
                serializedAdditionalRawData: null,
                dataSources?.ToList());
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.CreateChatCompletionResponse"/>. </summary>
        /// <param name="id"> A unique identifier for the chat completion. </param>
        /// <param name="choices"> A list of chat completion choices. Can be more than one if `n` is greater than 1. </param>
        /// <param name="created"> The Unix timestamp (in seconds) of when the chat completion was created. </param>
        /// <param name="model"> The model used for the chat completion. </param>
        /// <param name="systemFingerprint">
        /// This fingerprint represents the backend configuration that the model runs with.
        ///
        /// Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.
        /// </param>
        /// <param name="object"> The object type, which is always `chat.completion`. </param>
        /// <param name="usage"></param>
        /// <returns> A new <see cref="OpenAI.CreateChatCompletionResponse"/> instance for mocking. </returns>
        public static CreateChatCompletionResponse CreateChatCompletionResponse(string id = null, IEnumerable<CreateChatCompletionResponseChoice> choices = null, DateTimeOffset created = default, string model = null, string systemFingerprint = null, string @object = null, CompletionUsage usage = null)
        {
            choices ??= new List<CreateChatCompletionResponseChoice>();

            return new CreateChatCompletionResponse(
                id,
                choices?.ToList(),
                created,
                model,
                systemFingerprint,
                @object,
                usage,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.CreateChatCompletionResponseChoice"/>. </summary>
        /// <param name="finishReason">
        /// The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence,
        /// `length` if the maximum number of tokens specified in the request was reached,
        /// `content_filter` if content was omitted due to a flag from our content filters,
        /// `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model called a function.
        /// </param>
        /// <param name="index"> The index of the choice in the list of choices. </param>
        /// <param name="message"></param>
        /// <param name="logprobs"> Log probability information for the choice. </param>
        /// <returns> A new <see cref="OpenAI.CreateChatCompletionResponseChoice"/> instance for mocking. </returns>
        public static CreateChatCompletionResponseChoice CreateChatCompletionResponseChoice(string finishReason = null, int index = default, ChatCompletionResponseMessage message = null, CreateChatCompletionResponseChoiceLogprobs logprobs = null)
        {
            return new CreateChatCompletionResponseChoice(finishReason, index, message, logprobs, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionResponseMessage"/>. </summary>
        /// <param name="content"> The contents of the message. </param>
        /// <param name="toolCalls"></param>
        /// <param name="role"> The role of the author of this message. </param>
        /// <param name="functionCall"> Deprecated and replaced by `tool_calls`. The name and arguments of a function that should be called, as generated by the model. </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionResponseMessage"/> instance for mocking. </returns>
        public static ChatCompletionResponseMessage ChatCompletionResponseMessage(string content = null, IEnumerable<ChatCompletionMessageToolCall> toolCalls = null, string role = null, ChatCompletionResponseMessageFunctionCall functionCall = null)
        {
            toolCalls ??= new List<ChatCompletionMessageToolCall>();

            return new ChatCompletionResponseMessage(content, toolCalls?.ToList(), role, functionCall, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionResponseMessageFunctionCall"/>. </summary>
        /// <param name="arguments"> The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function. </param>
        /// <param name="name"> The name of the function to call. </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionResponseMessageFunctionCall"/> instance for mocking. </returns>
        public static ChatCompletionResponseMessageFunctionCall ChatCompletionResponseMessageFunctionCall(string arguments = null, string name = null)
        {
            return new ChatCompletionResponseMessageFunctionCall(arguments, name, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.CreateChatCompletionResponseChoiceLogprobs"/>. </summary>
        /// <param name="content"> A list of message content tokens with log probability information. </param>
        /// <returns> A new <see cref="OpenAI.CreateChatCompletionResponseChoiceLogprobs"/> instance for mocking. </returns>
        public static CreateChatCompletionResponseChoiceLogprobs CreateChatCompletionResponseChoiceLogprobs(IEnumerable<ChatCompletionTokenLogprob> content = null)
        {
            content ??= new List<ChatCompletionTokenLogprob>();

            return new CreateChatCompletionResponseChoiceLogprobs(content?.ToList(), serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionTokenLogprob"/>. </summary>
        /// <param name="token"> The token. </param>
        /// <param name="logprob"> The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value `-9999.0` is used to signify that the token is very unlikely. </param>
        /// <param name="bytes"> A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be `null` if there is no bytes representation for the token. </param>
        /// <param name="topLogprobs"> List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested `top_logprobs` returned. </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionTokenLogprob"/> instance for mocking. </returns>
        public static ChatCompletionTokenLogprob ChatCompletionTokenLogprob(string token = null, float logprob = default, IEnumerable<int> bytes = null, IEnumerable<ChatCompletionTokenLogprobTopLogprob> topLogprobs = null)
        {
            bytes ??= new List<int>();
            topLogprobs ??= new List<ChatCompletionTokenLogprobTopLogprob>();

            return new ChatCompletionTokenLogprob(token, logprob, bytes?.ToList(), topLogprobs?.ToList(), serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionTokenLogprobTopLogprob"/>. </summary>
        /// <param name="token"> The token. </param>
        /// <param name="logprob"> The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value `-9999.0` is used to signify that the token is very unlikely. </param>
        /// <param name="bytes"> A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be `null` if there is no bytes representation for the token. </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionTokenLogprobTopLogprob"/> instance for mocking. </returns>
        public static ChatCompletionTokenLogprobTopLogprob ChatCompletionTokenLogprobTopLogprob(string token = null, float logprob = default, IEnumerable<int> bytes = null)
        {
            bytes ??= new List<int>();

            return new ChatCompletionTokenLogprobTopLogprob(token, logprob, bytes?.ToList(), serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.CompletionUsage"/>. </summary>
        /// <param name="completionTokens"> Number of tokens in the generated completion. </param>
        /// <param name="promptTokens"> Number of tokens in the prompt. </param>
        /// <param name="totalTokens"> Total number of tokens used in the request (prompt + completion). </param>
        /// <returns> A new <see cref="OpenAI.CompletionUsage"/> instance for mocking. </returns>
        public static CompletionUsage CompletionUsage(int completionTokens = default, int promptTokens = default, int totalTokens = default)
        {
            return new CompletionUsage(completionTokens, promptTokens, totalTokens, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureCreateChatCompletionResponse"/>. </summary>
        /// <param name="id"> A unique identifier for the chat completion. </param>
        /// <param name="choices"> A list of chat completion choices. Can be more than one if `n` is greater than 1. </param>
        /// <param name="created"> The Unix timestamp (in seconds) of when the chat completion was created. </param>
        /// <param name="model"> The model used for the chat completion. </param>
        /// <param name="systemFingerprint">
        /// This fingerprint represents the backend configuration that the model runs with.
        ///
        /// Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.
        /// </param>
        /// <param name="object"> The object type, which is always `chat.completion`. </param>
        /// <param name="usage"></param>
        /// <param name="promptFilterResults"></param>
        /// <returns> A new <see cref="OpenAI.AzureCreateChatCompletionResponse"/> instance for mocking. </returns>
        public static AzureCreateChatCompletionResponse AzureCreateChatCompletionResponse(string id = null, IEnumerable<CreateChatCompletionResponseChoice> choices = null, DateTimeOffset created = default, string model = null, string systemFingerprint = null, string @object = null, CompletionUsage usage = null, IEnumerable<AzureCreateChatCompletionResponsePromptFilterResult> promptFilterResults = null)
        {
            choices ??= new List<CreateChatCompletionResponseChoice>();
            promptFilterResults ??= new List<AzureCreateChatCompletionResponsePromptFilterResult>();

            return new AzureCreateChatCompletionResponse(
                id,
                choices?.ToList(),
                created,
                model,
                systemFingerprint,
                @object,
                usage,
                serializedAdditionalRawData: null,
                promptFilterResults?.ToList());
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureCreateChatCompletionResponsePromptFilterResult"/>. </summary>
        /// <param name="promptIndex"></param>
        /// <param name="contentFilterResults"></param>
        /// <returns> A new <see cref="OpenAI.AzureCreateChatCompletionResponsePromptFilterResult"/> instance for mocking. </returns>
        public static AzureCreateChatCompletionResponsePromptFilterResult AzureCreateChatCompletionResponsePromptFilterResult(int promptIndex = default, IEnumerable<AzureCreateChatCompletionResponsePromptFilterResultContentFilterResult> contentFilterResults = null)
        {
            contentFilterResults ??= new List<AzureCreateChatCompletionResponsePromptFilterResultContentFilterResult>();

            return new AzureCreateChatCompletionResponsePromptFilterResult(promptIndex, contentFilterResults?.ToList(), serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureCreateChatCompletionResponsePromptFilterResultContentFilterResult"/>. </summary>
        /// <param name="sexual"></param>
        /// <param name="violence"></param>
        /// <param name="hate"></param>
        /// <param name="selfHarm"></param>
        /// <param name="profanity"></param>
        /// <param name="customBlocklists"></param>
        /// <param name="error">
        /// Describes an error returned if the content filtering system is
        /// down or otherwise unable to complete the operation in time.
        /// </param>
        /// <param name="jailbreak"></param>
        /// <returns> A new <see cref="OpenAI.AzureCreateChatCompletionResponsePromptFilterResultContentFilterResult"/> instance for mocking. </returns>
        public static AzureCreateChatCompletionResponsePromptFilterResultContentFilterResult AzureCreateChatCompletionResponsePromptFilterResultContentFilterResult(AzureContentFilterResult sexual = null, AzureContentFilterResult violence = null, AzureContentFilterResult hate = null, AzureContentFilterResult selfHarm = null, AzureContentFilterDetectionResult profanity = null, IEnumerable<AzureContentFilterBlocklistIdResult> customBlocklists = null, IReadOnlyDictionary<string, BinaryData> error = null, AzureContentFilterDetectionResult jailbreak = null)
        {
            customBlocklists ??= new List<AzureContentFilterBlocklistIdResult>();
            error ??= new Dictionary<string, BinaryData>();

            return new AzureCreateChatCompletionResponsePromptFilterResultContentFilterResult(
                sexual,
                violence,
                hate,
                selfHarm,
                profanity,
                customBlocklists?.ToList(),
                error,
                jailbreak,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureContentFilterResult"/>. </summary>
        /// <param name="severity"></param>
        /// <param name="filtered"></param>
        /// <returns> A new <see cref="OpenAI.AzureContentFilterResult"/> instance for mocking. </returns>
        public static AzureContentFilterResult AzureContentFilterResult(string severity = null, bool filtered = default)
        {
            return new AzureContentFilterResult(severity, filtered, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureContentFilterDetectionResult"/>. </summary>
        /// <param name="detected"></param>
        /// <param name="filtered"></param>
        /// <returns> A new <see cref="OpenAI.AzureContentFilterDetectionResult"/> instance for mocking. </returns>
        public static AzureContentFilterDetectionResult AzureContentFilterDetectionResult(bool detected = default, bool filtered = default)
        {
            return new AzureContentFilterDetectionResult(detected, filtered, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureContentFilterBlocklistIdResult"/>. </summary>
        /// <param name="id"></param>
        /// <param name="filtered"></param>
        /// <returns> A new <see cref="OpenAI.AzureContentFilterBlocklistIdResult"/> instance for mocking. </returns>
        public static AzureContentFilterBlocklistIdResult AzureContentFilterBlocklistIdResult(string id = null, bool filtered = default)
        {
            return new AzureContentFilterBlocklistIdResult(id, filtered, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatSearchDataSource"/>. </summary>
        /// <param name="parameters"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatSearchDataSource"/> instance for mocking. </returns>
        public static AzureChatSearchDataSource AzureChatSearchDataSource(AzureChatSearchDataSourceParameters parameters = null)
        {
            return new AzureChatSearchDataSource("azure_search", serializedAdditionalRawData: null, parameters);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatSearchDataSourceParameters"/>. </summary>
        /// <param name="authentication"></param>
        /// <param name="topNDocuments"></param>
        /// <param name="inScope"></param>
        /// <param name="strictness"></param>
        /// <param name="roleInformation"></param>
        /// <param name="endpoint"></param>
        /// <param name="indexName"></param>
        /// <param name="fieldsMapping"></param>
        /// <param name="queryType"></param>
        /// <param name="semanticConfiguration"></param>
        /// <param name="filter"></param>
        /// <param name="embeddingDependency">
        /// Please note <see cref="AzureChatDataSourceVectorizationSource"/> is the base class. According to the scenario, a derived class of the base class might need to be assigned here, or this property needs to be casted to one of the possible derived classes.
        /// The available derived classes include <see cref="OpenAI.AzureChatDataSourceDeploymentNameVectorizationSource"/>, <see cref="OpenAI.AzureChatDataSourceEndpointVectorizationSource"/> and <see cref="OpenAI.AzureChatDataSourceModelIdVectorizationSource"/>.
        /// </param>
        /// <returns> A new <see cref="OpenAI.AzureChatSearchDataSourceParameters"/> instance for mocking. </returns>
        public static AzureChatSearchDataSourceParameters AzureChatSearchDataSourceParameters(BinaryData authentication = null, int? topNDocuments = null, bool? inScope = null, int? strictness = null, string roleInformation = null, Uri endpoint = null, string indexName = null, AzureChatSearchDataSourceParametersFieldsMapping fieldsMapping = null, string queryType = null, string semanticConfiguration = null, string filter = null, AzureChatDataSourceVectorizationSource embeddingDependency = null)
        {
            return new AzureChatSearchDataSourceParameters(
                authentication,
                topNDocuments,
                inScope,
                strictness,
                roleInformation,
                endpoint,
                indexName,
                fieldsMapping,
                queryType,
                semanticConfiguration,
                filter,
                embeddingDependency,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatSearchDataSourceParametersFieldsMapping"/>. </summary>
        /// <param name="titleField"></param>
        /// <param name="urlField"></param>
        /// <param name="filepathField"></param>
        /// <param name="contentFields"></param>
        /// <param name="contentFieldsSeparator"></param>
        /// <param name="vectorFields"></param>
        /// <param name="imageVectorFields"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatSearchDataSourceParametersFieldsMapping"/> instance for mocking. </returns>
        public static AzureChatSearchDataSourceParametersFieldsMapping AzureChatSearchDataSourceParametersFieldsMapping(string titleField = null, string urlField = null, string filepathField = null, IEnumerable<string> contentFields = null, string contentFieldsSeparator = null, IEnumerable<string> vectorFields = null, IEnumerable<string> imageVectorFields = null)
        {
            contentFields ??= new List<string>();
            vectorFields ??= new List<string>();
            imageVectorFields ??= new List<string>();

            return new AzureChatSearchDataSourceParametersFieldsMapping(
                titleField,
                urlField,
                filepathField,
                contentFields?.ToList(),
                contentFieldsSeparator,
                vectorFields?.ToList(),
                imageVectorFields?.ToList(),
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatCosmosDBDataSource"/>. </summary>
        /// <param name="parameters"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatCosmosDBDataSource"/> instance for mocking. </returns>
        public static AzureChatCosmosDBDataSource AzureChatCosmosDBDataSource(AzureChatCosmosDBDataSourceParameters parameters = null)
        {
            return new AzureChatCosmosDBDataSource("AzureCosmosDB", serializedAdditionalRawData: null, parameters);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatCosmosDBDataSourceParameters"/>. </summary>
        /// <param name="authentication"></param>
        /// <param name="topNDocuments"></param>
        /// <param name="inScope"></param>
        /// <param name="strictness"></param>
        /// <param name="roleInformation"></param>
        /// <param name="databaseName"></param>
        /// <param name="containerName"></param>
        /// <param name="indexName"></param>
        /// <param name="fieldsMapping"></param>
        /// <param name="embeddingDependency">
        /// Please note <see cref="AzureChatDataSourceVectorizationSource"/> is the base class. According to the scenario, a derived class of the base class might need to be assigned here, or this property needs to be casted to one of the possible derived classes.
        /// The available derived classes include <see cref="OpenAI.AzureChatDataSourceDeploymentNameVectorizationSource"/>, <see cref="OpenAI.AzureChatDataSourceEndpointVectorizationSource"/> and <see cref="OpenAI.AzureChatDataSourceModelIdVectorizationSource"/>.
        /// </param>
        /// <returns> A new <see cref="OpenAI.AzureChatCosmosDBDataSourceParameters"/> instance for mocking. </returns>
        public static AzureChatCosmosDBDataSourceParameters AzureChatCosmosDBDataSourceParameters(BinaryData authentication = null, int? topNDocuments = null, bool? inScope = null, int? strictness = null, string roleInformation = null, string databaseName = null, string containerName = null, string indexName = null, AzureChatCosmosDBDataSourceParametersFieldsMapping fieldsMapping = null, AzureChatDataSourceVectorizationSource embeddingDependency = null)
        {
            return new AzureChatCosmosDBDataSourceParameters(
                authentication,
                topNDocuments,
                inScope,
                strictness,
                roleInformation,
                databaseName,
                containerName,
                indexName,
                fieldsMapping,
                embeddingDependency,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatCosmosDBDataSourceParametersFieldsMapping"/>. </summary>
        /// <param name="titleField"></param>
        /// <param name="urlField"></param>
        /// <param name="filepathField"></param>
        /// <param name="contentFields"></param>
        /// <param name="contentFieldsSeparator"></param>
        /// <param name="vectorFields"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatCosmosDBDataSourceParametersFieldsMapping"/> instance for mocking. </returns>
        public static AzureChatCosmosDBDataSourceParametersFieldsMapping AzureChatCosmosDBDataSourceParametersFieldsMapping(string titleField = null, string urlField = null, string filepathField = null, IEnumerable<string> contentFields = null, string contentFieldsSeparator = null, IEnumerable<string> vectorFields = null)
        {
            contentFields ??= new List<string>();
            vectorFields ??= new List<string>();

            return new AzureChatCosmosDBDataSourceParametersFieldsMapping(
                titleField,
                urlField,
                filepathField,
                contentFields?.ToList(),
                contentFieldsSeparator,
                vectorFields?.ToList(),
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatMachineLearningIndexDataSource"/>. </summary>
        /// <param name="parameters"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatMachineLearningIndexDataSource"/> instance for mocking. </returns>
        public static AzureChatMachineLearningIndexDataSource AzureChatMachineLearningIndexDataSource(AzureChatMachineLearningIndexDataSourceParameters parameters = null)
        {
            return new AzureChatMachineLearningIndexDataSource("azure_ml_index", serializedAdditionalRawData: null, parameters);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatMachineLearningIndexDataSourceParameters"/>. </summary>
        /// <param name="authentication"></param>
        /// <param name="topNDocuments"></param>
        /// <param name="inScope"></param>
        /// <param name="strictness"></param>
        /// <param name="roleInformation"></param>
        /// <param name="projectResourceId"></param>
        /// <param name="name"></param>
        /// <param name="version"></param>
        /// <param name="filter"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatMachineLearningIndexDataSourceParameters"/> instance for mocking. </returns>
        public static AzureChatMachineLearningIndexDataSourceParameters AzureChatMachineLearningIndexDataSourceParameters(BinaryData authentication = null, int? topNDocuments = null, bool? inScope = null, int? strictness = null, string roleInformation = null, string projectResourceId = null, string name = null, string version = null, string filter = null)
        {
            return new AzureChatMachineLearningIndexDataSourceParameters(
                authentication,
                topNDocuments,
                inScope,
                strictness,
                roleInformation,
                projectResourceId,
                name,
                version,
                filter,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatElasticsearchDataSource"/>. </summary>
        /// <param name="parameters"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatElasticsearchDataSource"/> instance for mocking. </returns>
        public static AzureChatElasticsearchDataSource AzureChatElasticsearchDataSource(AzureChatElasticsearchDataSourceParameters parameters = null)
        {
            return new AzureChatElasticsearchDataSource("elasticsearch", serializedAdditionalRawData: null, parameters);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatElasticsearchDataSourceParameters"/>. </summary>
        /// <param name="authentication"></param>
        /// <param name="topNDocuments"></param>
        /// <param name="inScope"></param>
        /// <param name="strictness"></param>
        /// <param name="roleInformation"></param>
        /// <param name="endpoint"></param>
        /// <param name="indexName"></param>
        /// <param name="fieldsMapping"></param>
        /// <param name="queryType"></param>
        /// <param name="embeddingDependency">
        /// Please note <see cref="AzureChatDataSourceVectorizationSource"/> is the base class. According to the scenario, a derived class of the base class might need to be assigned here, or this property needs to be casted to one of the possible derived classes.
        /// The available derived classes include <see cref="OpenAI.AzureChatDataSourceDeploymentNameVectorizationSource"/>, <see cref="OpenAI.AzureChatDataSourceEndpointVectorizationSource"/> and <see cref="OpenAI.AzureChatDataSourceModelIdVectorizationSource"/>.
        /// </param>
        /// <returns> A new <see cref="OpenAI.AzureChatElasticsearchDataSourceParameters"/> instance for mocking. </returns>
        public static AzureChatElasticsearchDataSourceParameters AzureChatElasticsearchDataSourceParameters(BinaryData authentication = null, int? topNDocuments = null, bool? inScope = null, int? strictness = null, string roleInformation = null, Uri endpoint = null, string indexName = null, AzureChatElasticsearchDataSourceParametersFieldsMapping fieldsMapping = null, string queryType = null, AzureChatDataSourceVectorizationSource embeddingDependency = null)
        {
            return new AzureChatElasticsearchDataSourceParameters(
                authentication,
                topNDocuments,
                inScope,
                strictness,
                roleInformation,
                endpoint,
                indexName,
                fieldsMapping,
                queryType,
                embeddingDependency,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatElasticsearchDataSourceParametersFieldsMapping"/>. </summary>
        /// <param name="titleField"></param>
        /// <param name="urlField"></param>
        /// <param name="filepathField"></param>
        /// <param name="contentFields"></param>
        /// <param name="contentFieldsSeparator"></param>
        /// <param name="vectorFields"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatElasticsearchDataSourceParametersFieldsMapping"/> instance for mocking. </returns>
        public static AzureChatElasticsearchDataSourceParametersFieldsMapping AzureChatElasticsearchDataSourceParametersFieldsMapping(string titleField = null, string urlField = null, string filepathField = null, IEnumerable<string> contentFields = null, string contentFieldsSeparator = null, IEnumerable<string> vectorFields = null)
        {
            contentFields ??= new List<string>();
            vectorFields ??= new List<string>();

            return new AzureChatElasticsearchDataSourceParametersFieldsMapping(
                titleField,
                urlField,
                filepathField,
                contentFields?.ToList(),
                contentFieldsSeparator,
                vectorFields?.ToList(),
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatPineconeDataSource"/>. </summary>
        /// <param name="authentication"></param>
        /// <param name="topNDocuments"></param>
        /// <param name="inScope"></param>
        /// <param name="strictness"></param>
        /// <param name="roleInformation"></param>
        /// <param name="environment"></param>
        /// <param name="indexName"></param>
        /// <param name="fieldsMapping"></param>
        /// <param name="embeddingDependency">
        /// Please note <see cref="AzureChatDataSourceVectorizationSource"/> is the base class. According to the scenario, a derived class of the base class might need to be assigned here, or this property needs to be casted to one of the possible derived classes.
        /// The available derived classes include <see cref="OpenAI.AzureChatDataSourceDeploymentNameVectorizationSource"/>, <see cref="OpenAI.AzureChatDataSourceEndpointVectorizationSource"/> and <see cref="OpenAI.AzureChatDataSourceModelIdVectorizationSource"/>.
        /// </param>
        /// <returns> A new <see cref="OpenAI.AzureChatPineconeDataSource"/> instance for mocking. </returns>
        public static AzureChatPineconeDataSource AzureChatPineconeDataSource(BinaryData authentication = null, int? topNDocuments = null, bool? inScope = null, int? strictness = null, string roleInformation = null, string environment = null, string indexName = null, AzureChatPineconeDataSourceFieldsMapping fieldsMapping = null, AzureChatDataSourceVectorizationSource embeddingDependency = null)
        {
            return new AzureChatPineconeDataSource(
                "pinecone",
                serializedAdditionalRawData: null,
                authentication,
                topNDocuments,
                inScope,
                strictness,
                roleInformation,
                environment,
                indexName,
                fieldsMapping,
                embeddingDependency);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatPineconeDataSourceFieldsMapping"/>. </summary>
        /// <param name="titleField"></param>
        /// <param name="urlField"></param>
        /// <param name="filepathField"></param>
        /// <param name="contentFields"></param>
        /// <param name="contentFieldsSeparator"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatPineconeDataSourceFieldsMapping"/> instance for mocking. </returns>
        public static AzureChatPineconeDataSourceFieldsMapping AzureChatPineconeDataSourceFieldsMapping(string titleField = null, string urlField = null, string filepathField = null, IEnumerable<string> contentFields = null, string contentFieldsSeparator = null)
        {
            contentFields ??= new List<string>();

            return new AzureChatPineconeDataSourceFieldsMapping(
                titleField,
                urlField,
                filepathField,
                contentFields?.ToList(),
                contentFieldsSeparator,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatDataSourceApiKeyAuthenticationOptions"/>. </summary>
        /// <param name="key"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatDataSourceApiKeyAuthenticationOptions"/> instance for mocking. </returns>
        public static AzureChatDataSourceApiKeyAuthenticationOptions AzureChatDataSourceApiKeyAuthenticationOptions(string key = null)
        {
            return new AzureChatDataSourceApiKeyAuthenticationOptions("api_key", serializedAdditionalRawData: null, key);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatDataSourceEndpointVectorizationSource"/>. </summary>
        /// <param name="endpoint"></param>
        /// <param name="authentication">
        /// Please note <see cref="AzureChatDataSourceAuthenticationOptions"/> is the base class. According to the scenario, a derived class of the base class might need to be assigned here, or this property needs to be casted to one of the possible derived classes.
        /// The available derived classes include <see cref="OpenAI.AzureChatDataSourceApiKeyAuthenticationOptions"/>.
        /// </param>
        /// <returns> A new <see cref="OpenAI.AzureChatDataSourceEndpointVectorizationSource"/> instance for mocking. </returns>
        public static AzureChatDataSourceEndpointVectorizationSource AzureChatDataSourceEndpointVectorizationSource(Uri endpoint = null, AzureChatDataSourceAuthenticationOptions authentication = null)
        {
            return new AzureChatDataSourceEndpointVectorizationSource("endpoint", serializedAdditionalRawData: null, endpoint, authentication);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatDataSourceDeploymentNameVectorizationSource"/>. </summary>
        /// <param name="deploymentName"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatDataSourceDeploymentNameVectorizationSource"/> instance for mocking. </returns>
        public static AzureChatDataSourceDeploymentNameVectorizationSource AzureChatDataSourceDeploymentNameVectorizationSource(string deploymentName = null)
        {
            return new AzureChatDataSourceDeploymentNameVectorizationSource("deployment_name", serializedAdditionalRawData: null, deploymentName);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatDataSourceModelIdVectorizationSource"/>. </summary>
        /// <param name="modelId"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatDataSourceModelIdVectorizationSource"/> instance for mocking. </returns>
        public static AzureChatDataSourceModelIdVectorizationSource AzureChatDataSourceModelIdVectorizationSource(string modelId = null)
        {
            return new AzureChatDataSourceModelIdVectorizationSource("model_id", serializedAdditionalRawData: null, modelId);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatCompletionResponseMessage"/>. </summary>
        /// <param name="content"> The contents of the message. </param>
        /// <param name="toolCalls"></param>
        /// <param name="role"> The role of the author of this message. </param>
        /// <param name="functionCall"> Deprecated and replaced by `tool_calls`. The name and arguments of a function that should be called, as generated by the model. </param>
        /// <param name="context"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatCompletionResponseMessage"/> instance for mocking. </returns>
        public static AzureChatCompletionResponseMessage AzureChatCompletionResponseMessage(string content = null, IEnumerable<ChatCompletionMessageToolCall> toolCalls = null, string role = null, ChatCompletionResponseMessageFunctionCall functionCall = null, AzureChatCompletionResponseMessageContext context = null)
        {
            toolCalls ??= new List<ChatCompletionMessageToolCall>();

            return new AzureChatCompletionResponseMessage(
                content,
                toolCalls?.ToList(),
                role,
                functionCall,
                serializedAdditionalRawData: null,
                context);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatCompletionResponseMessageContext"/>. </summary>
        /// <param name="intent"></param>
        /// <param name="citations"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatCompletionResponseMessageContext"/> instance for mocking. </returns>
        public static AzureChatCompletionResponseMessageContext AzureChatCompletionResponseMessageContext(string intent = null, IEnumerable<AzureChatCompletionResponseMessageContextCitation> citations = null)
        {
            citations ??= new List<AzureChatCompletionResponseMessageContextCitation>();

            return new AzureChatCompletionResponseMessageContext(intent, citations?.ToList(), serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatCompletionResponseMessageContextCitation"/>. </summary>
        /// <param name="content"></param>
        /// <param name="title"></param>
        /// <param name="url"></param>
        /// <param name="filepath"></param>
        /// <param name="chunkId"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatCompletionResponseMessageContextCitation"/> instance for mocking. </returns>
        public static AzureChatCompletionResponseMessageContextCitation AzureChatCompletionResponseMessageContextCitation(string content = null, string title = null, string url = null, string filepath = null, string chunkId = null)
        {
            return new AzureChatCompletionResponseMessageContextCitation(
                content,
                title,
                url,
                filepath,
                chunkId,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureContentFilterCitedDetectionResult"/>. </summary>
        /// <param name="detected"></param>
        /// <param name="filtered"></param>
        /// <param name="license"></param>
        /// <param name="url"></param>
        /// <returns> A new <see cref="OpenAI.AzureContentFilterCitedDetectionResult"/> instance for mocking. </returns>
        public static AzureContentFilterCitedDetectionResult AzureContentFilterCitedDetectionResult(bool detected = default, bool filtered = default, string license = null, Uri url = null)
        {
            return new AzureContentFilterCitedDetectionResult(detected, filtered, license, url, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureContentFilterResultForChoice"/>. </summary>
        /// <param name="sexual"></param>
        /// <param name="violence"></param>
        /// <param name="hate"></param>
        /// <param name="selfHarm"></param>
        /// <param name="profanity"></param>
        /// <param name="customBlocklists"></param>
        /// <param name="error">
        /// Describes an error returned if the content filtering system is
        /// down or otherwise unable to complete the operation in time.
        /// </param>
        /// <param name="protectedMaterialText"></param>
        /// <param name="protectedMaterialCode"></param>
        /// <returns> A new <see cref="OpenAI.AzureContentFilterResultForChoice"/> instance for mocking. </returns>
        public static AzureContentFilterResultForChoice AzureContentFilterResultForChoice(AzureContentFilterResult sexual = null, AzureContentFilterResult violence = null, AzureContentFilterResult hate = null, AzureContentFilterResult selfHarm = null, AzureContentFilterDetectionResult profanity = null, IEnumerable<AzureContentFilterBlocklistIdResult> customBlocklists = null, IReadOnlyDictionary<string, BinaryData> error = null, AzureContentFilterDetectionResult protectedMaterialText = null, AzureContentFilterCitedDetectionResult protectedMaterialCode = null)
        {
            customBlocklists ??= new List<AzureContentFilterBlocklistIdResult>();
            error ??= new Dictionary<string, BinaryData>();

            return new AzureContentFilterResultForChoice(
                sexual,
                violence,
                hate,
                selfHarm,
                profanity,
                customBlocklists?.ToList(),
                error,
                protectedMaterialText,
                protectedMaterialCode,
                serializedAdditionalRawData: null);
        }
    }
}
