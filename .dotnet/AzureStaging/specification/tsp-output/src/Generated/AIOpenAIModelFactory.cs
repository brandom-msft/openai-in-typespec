// <auto-generated/>

#nullable disable

using System;
using System.Collections.Generic;
using System.Linq;

namespace Azure.AI.OpenAI
{
    /// <summary> Model factory for models. </summary>
    public static partial class AIOpenAIModelFactory
    {
        /// <summary> Initializes a new instance of <see cref="OpenAI.CreateChatCompletionRequest"/>. </summary>
        /// <param name="messages"> A list of messages comprising the conversation so far. [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models). </param>
        /// <param name="model"> ID of the model to use. See the [model endpoint compatibility](/docs/models/model-endpoint-compatibility) table for details on which models work with the Chat API. </param>
        /// <param name="frequencyPenalty">
        /// Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
        ///
        /// [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)
        /// </param>
        /// <param name="logitBias">
        /// Modify the likelihood of specified tokens appearing in the completion.
        ///
        /// Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
        /// </param>
        /// <param name="logprobs"> Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`. </param>
        /// <param name="topLogprobs"> An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used. </param>
        /// <param name="maxTokens">
        /// The maximum number of [tokens](/tokenizer) that can be generated in the chat completion.
        ///
        /// The total length of input tokens and generated tokens is limited by the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens.
        /// </param>
        /// <param name="n"> How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs. </param>
        /// <param name="presencePenalty">
        /// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
        ///
        /// [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)
        /// </param>
        /// <param name="responseFormat">
        /// An object specifying the format that the model must output. Compatible with [GPT-4 Turbo](/docs/models/gpt-4-and-gpt-4-turbo) and all GPT-3.5 Turbo models newer than `gpt-3.5-turbo-1106`.
        ///
        /// Setting to `{ "type": "json_object" }` enables JSON mode, which guarantees the message the model generates is valid JSON.
        ///
        /// **Important:** when using JSON mode, you **must** also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly "stuck" request. Also note that the message content may be partially cut off if `finish_reason="length"`, which indicates the generation exceeded `max_tokens` or the conversation exceeded the max context length.
        /// </param>
        /// <param name="seed">
        /// This feature is in Beta.
        /// If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.
        /// Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.
        /// </param>
        /// <param name="stop"> Up to 4 sequences where the API will stop generating further tokens. </param>
        /// <param name="stream"> If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions). </param>
        /// <param name="streamOptions"></param>
        /// <param name="temperature">
        /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
        ///
        /// We generally recommend altering this or `top_p` but not both.
        /// </param>
        /// <param name="topP">
        /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
        ///
        /// We generally recommend altering this or `temperature` but not both.
        /// </param>
        /// <param name="tools"> A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported. </param>
        /// <param name="toolChoice"></param>
        /// <param name="user"> A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids). </param>
        /// <param name="functionCall">
        /// Deprecated in favor of `tool_choice`.
        ///
        /// Controls which (if any) function is called by the model.
        /// `none` means the model will not call a function and instead generates a message.
        /// `auto` means the model can pick between generating a message or calling a function.
        /// Specifying a particular function via `{"name": "my_function"}` forces the model to call that function.
        ///
        /// `none` is the default when no functions are present. `auto` is the default if functions are present.
        /// </param>
        /// <param name="functions">
        /// Deprecated in favor of `tools`.
        ///
        /// A list of functions the model may generate JSON inputs for.
        /// </param>
        /// <returns> A new <see cref="OpenAI.CreateChatCompletionRequest"/> instance for mocking. </returns>
        public static CreateChatCompletionRequest CreateChatCompletionRequest(IEnumerable<BinaryData> messages = null, string model = null, float? frequencyPenalty = null, IReadOnlyDictionary<string, int> logitBias = null, bool? logprobs = null, int? topLogprobs = null, int? maxTokens = null, int? n = null, float? presencePenalty = null, CreateChatCompletionRequestResponseFormat responseFormat = null, long? seed = null, BinaryData stop = null, bool? stream = null, ChatCompletionStreamOptions streamOptions = null, float? temperature = null, float? topP = null, IEnumerable<ChatCompletionTool> tools = null, BinaryData toolChoice = null, string user = null, BinaryData functionCall = null, IEnumerable<ChatCompletionFunctions> functions = null)
        {
            messages ??= new List<BinaryData>();
            logitBias ??= new Dictionary<string, int>();
            tools ??= new List<ChatCompletionTool>();
            functions ??= new List<ChatCompletionFunctions>();

            return new CreateChatCompletionRequest(
                messages?.ToList(),
                model,
                frequencyPenalty,
                logitBias,
                logprobs,
                topLogprobs,
                maxTokens,
                n,
                presencePenalty,
                responseFormat,
                seed,
                stop,
                stream,
                streamOptions,
                temperature,
                topP,
                tools?.ToList(),
                toolChoice,
                user,
                functionCall,
                functions?.ToList(),
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionRequestSystemMessage"/>. </summary>
        /// <param name="content"> The contents of the system message. </param>
        /// <param name="role"> The role of the messages author, in this case `system`. </param>
        /// <param name="name"> An optional name for the participant. Provides the model information to differentiate between participants of the same role. </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionRequestSystemMessage"/> instance for mocking. </returns>
        public static ChatCompletionRequestSystemMessage ChatCompletionRequestSystemMessage(string content = null, string role = null, string name = null)
        {
            return new ChatCompletionRequestSystemMessage(content, role, name, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionRequestUserMessage"/>. </summary>
        /// <param name="content"> The contents of the user message. </param>
        /// <param name="role"> The role of the messages author, in this case `user`. </param>
        /// <param name="name"> An optional name for the participant. Provides the model information to differentiate between participants of the same role. </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionRequestUserMessage"/> instance for mocking. </returns>
        public static ChatCompletionRequestUserMessage ChatCompletionRequestUserMessage(BinaryData content = null, string role = null, string name = null)
        {
            return new ChatCompletionRequestUserMessage(content, role, name, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionRequestMessageContentPartText"/>. </summary>
        /// <param name="type"> The type of the content part. </param>
        /// <param name="text"> The text content. </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionRequestMessageContentPartText"/> instance for mocking. </returns>
        public static ChatCompletionRequestMessageContentPartText ChatCompletionRequestMessageContentPartText(string type = null, string text = null)
        {
            return new ChatCompletionRequestMessageContentPartText(type, text, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionRequestMessageContentPartImage"/>. </summary>
        /// <param name="type"> The type of the content part. </param>
        /// <param name="imageUrl"></param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionRequestMessageContentPartImage"/> instance for mocking. </returns>
        public static ChatCompletionRequestMessageContentPartImage ChatCompletionRequestMessageContentPartImage(string type = null, ChatCompletionRequestMessageContentPartImageImageUrl imageUrl = null)
        {
            return new ChatCompletionRequestMessageContentPartImage(type, imageUrl, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionRequestMessageContentPartImageImageUrl"/>. </summary>
        /// <param name="url"> Either a URL of the image or the base64 encoded image data. </param>
        /// <param name="detail"> Specifies the detail level of the image. Learn more in the [Vision guide](/docs/guides/vision/low-or-high-fidelity-image-understanding). </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionRequestMessageContentPartImageImageUrl"/> instance for mocking. </returns>
        public static ChatCompletionRequestMessageContentPartImageImageUrl ChatCompletionRequestMessageContentPartImageImageUrl(Uri url = null, string detail = null)
        {
            return new ChatCompletionRequestMessageContentPartImageImageUrl(url, detail, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionRequestAssistantMessage"/>. </summary>
        /// <param name="content"> The contents of the assistant message. Required unless `tool_calls` or `function_call` is specified. </param>
        /// <param name="role"> The role of the messages author, in this case `assistant`. </param>
        /// <param name="name"> An optional name for the participant. Provides the model information to differentiate between participants of the same role. </param>
        /// <param name="toolCalls"></param>
        /// <param name="functionCall"> Deprecated and replaced by `tool_calls`. The name and arguments of a function that should be called, as generated by the model. </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionRequestAssistantMessage"/> instance for mocking. </returns>
        public static ChatCompletionRequestAssistantMessage ChatCompletionRequestAssistantMessage(string content = null, string role = null, string name = null, IEnumerable<ChatCompletionMessageToolCall> toolCalls = null, ChatCompletionRequestAssistantMessageFunctionCall functionCall = null)
        {
            toolCalls ??= new List<ChatCompletionMessageToolCall>();

            return new ChatCompletionRequestAssistantMessage(
                content,
                role,
                name,
                toolCalls?.ToList(),
                functionCall,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionMessageToolCall"/>. </summary>
        /// <param name="id"> The ID of the tool call. </param>
        /// <param name="type"> The type of the tool. Currently, only `function` is supported. </param>
        /// <param name="function"> The function that the model called. </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionMessageToolCall"/> instance for mocking. </returns>
        public static ChatCompletionMessageToolCall ChatCompletionMessageToolCall(string id = null, string type = null, ChatCompletionMessageToolCallFunction function = null)
        {
            return new ChatCompletionMessageToolCall(id, type, function, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionMessageToolCallFunction"/>. </summary>
        /// <param name="name"> The name of the function to call. </param>
        /// <param name="arguments"> The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function. </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionMessageToolCallFunction"/> instance for mocking. </returns>
        public static ChatCompletionMessageToolCallFunction ChatCompletionMessageToolCallFunction(string name = null, string arguments = null)
        {
            return new ChatCompletionMessageToolCallFunction(name, arguments, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionRequestAssistantMessageFunctionCall"/>. </summary>
        /// <param name="arguments"> The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function. </param>
        /// <param name="name"> The name of the function to call. </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionRequestAssistantMessageFunctionCall"/> instance for mocking. </returns>
        public static ChatCompletionRequestAssistantMessageFunctionCall ChatCompletionRequestAssistantMessageFunctionCall(string arguments = null, string name = null)
        {
            return new ChatCompletionRequestAssistantMessageFunctionCall(arguments, name, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionRequestToolMessage"/>. </summary>
        /// <param name="role"> The role of the messages author, in this case `tool`. </param>
        /// <param name="content"> The contents of the tool message. </param>
        /// <param name="toolCallId"> Tool call that this message is responding to. </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionRequestToolMessage"/> instance for mocking. </returns>
        public static ChatCompletionRequestToolMessage ChatCompletionRequestToolMessage(string role = null, string content = null, string toolCallId = null)
        {
            return new ChatCompletionRequestToolMessage(role, content, toolCallId, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionRequestFunctionMessage"/>. </summary>
        /// <param name="role"> The role of the messages author, in this case `function`. </param>
        /// <param name="content"> The contents of the function message. </param>
        /// <param name="name"> The name of the function to call. </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionRequestFunctionMessage"/> instance for mocking. </returns>
        public static ChatCompletionRequestFunctionMessage ChatCompletionRequestFunctionMessage(string role = null, string content = null, string name = null)
        {
            return new ChatCompletionRequestFunctionMessage(role, content, name, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.CreateChatCompletionRequestResponseFormat"/>. </summary>
        /// <param name="type"> Must be one of `text` or `json_object`. </param>
        /// <returns> A new <see cref="OpenAI.CreateChatCompletionRequestResponseFormat"/> instance for mocking. </returns>
        public static CreateChatCompletionRequestResponseFormat CreateChatCompletionRequestResponseFormat(string type = null)
        {
            return new CreateChatCompletionRequestResponseFormat(type, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionStreamOptions"/>. </summary>
        /// <param name="includeUsage"> If set, an additional chunk will be streamed before the `data: [DONE]` message. The `usage` field on this chunk shows the token usage statistics for the entire request, and the `choices` field will always be an empty array. All other chunks will also include a `usage` field, but with a null value. </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionStreamOptions"/> instance for mocking. </returns>
        public static ChatCompletionStreamOptions ChatCompletionStreamOptions(bool? includeUsage = null)
        {
            return new ChatCompletionStreamOptions(includeUsage, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionTool"/>. </summary>
        /// <param name="type"> The type of the tool. Currently, only `function` is supported. </param>
        /// <param name="function"></param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionTool"/> instance for mocking. </returns>
        public static ChatCompletionTool ChatCompletionTool(string type = null, FunctionObject function = null)
        {
            return new ChatCompletionTool(type, function, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.FunctionObject"/>. </summary>
        /// <param name="description"> A description of what the function does, used by the model to choose when and how to call the function. </param>
        /// <param name="name"> The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64. </param>
        /// <param name="parameters"></param>
        /// <returns> A new <see cref="OpenAI.FunctionObject"/> instance for mocking. </returns>
        public static FunctionObject FunctionObject(string description = null, string name = null, FunctionParameters parameters = null)
        {
            return new FunctionObject(description, name, parameters, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.FunctionParameters"/>. </summary>
        /// <param name="additionalProperties"> Additional Properties. </param>
        /// <returns> A new <see cref="OpenAI.FunctionParameters"/> instance for mocking. </returns>
        public static FunctionParameters FunctionParameters(IReadOnlyDictionary<string, BinaryData> additionalProperties = null)
        {
            additionalProperties ??= new Dictionary<string, BinaryData>();

            return new FunctionParameters(additionalProperties);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionNamedToolChoice"/>. </summary>
        /// <param name="type"> The type of the tool. Currently, only `function` is supported. </param>
        /// <param name="function"></param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionNamedToolChoice"/> instance for mocking. </returns>
        public static ChatCompletionNamedToolChoice ChatCompletionNamedToolChoice(string type = null, ChatCompletionNamedToolChoiceFunction function = null)
        {
            return new ChatCompletionNamedToolChoice(type, function, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionNamedToolChoiceFunction"/>. </summary>
        /// <param name="name"> The name of the function to call. </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionNamedToolChoiceFunction"/> instance for mocking. </returns>
        public static ChatCompletionNamedToolChoiceFunction ChatCompletionNamedToolChoiceFunction(string name = null)
        {
            return new ChatCompletionNamedToolChoiceFunction(name, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionFunctionCallOption"/>. </summary>
        /// <param name="name"> The name of the function to call. </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionFunctionCallOption"/> instance for mocking. </returns>
        public static ChatCompletionFunctionCallOption ChatCompletionFunctionCallOption(string name = null)
        {
            return new ChatCompletionFunctionCallOption(name, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionFunctions"/>. </summary>
        /// <param name="description"> A description of what the function does, used by the model to choose when and how to call the function. </param>
        /// <param name="name"> The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64. </param>
        /// <param name="parameters"></param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionFunctions"/> instance for mocking. </returns>
        public static ChatCompletionFunctions ChatCompletionFunctions(string description = null, string name = null, FunctionParameters parameters = null)
        {
            return new ChatCompletionFunctions(description, name, parameters, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureCreateChatCompletionRequest"/>. </summary>
        /// <param name="messages"> A list of messages comprising the conversation so far. [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models). </param>
        /// <param name="model"> ID of the model to use. See the [model endpoint compatibility](/docs/models/model-endpoint-compatibility) table for details on which models work with the Chat API. </param>
        /// <param name="frequencyPenalty">
        /// Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
        ///
        /// [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)
        /// </param>
        /// <param name="logitBias">
        /// Modify the likelihood of specified tokens appearing in the completion.
        ///
        /// Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
        /// </param>
        /// <param name="logprobs"> Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`. </param>
        /// <param name="topLogprobs"> An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used. </param>
        /// <param name="maxTokens">
        /// The maximum number of [tokens](/tokenizer) that can be generated in the chat completion.
        ///
        /// The total length of input tokens and generated tokens is limited by the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens.
        /// </param>
        /// <param name="n"> How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs. </param>
        /// <param name="presencePenalty">
        /// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
        ///
        /// [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)
        /// </param>
        /// <param name="responseFormat">
        /// An object specifying the format that the model must output. Compatible with [GPT-4 Turbo](/docs/models/gpt-4-and-gpt-4-turbo) and all GPT-3.5 Turbo models newer than `gpt-3.5-turbo-1106`.
        ///
        /// Setting to `{ "type": "json_object" }` enables JSON mode, which guarantees the message the model generates is valid JSON.
        ///
        /// **Important:** when using JSON mode, you **must** also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly "stuck" request. Also note that the message content may be partially cut off if `finish_reason="length"`, which indicates the generation exceeded `max_tokens` or the conversation exceeded the max context length.
        /// </param>
        /// <param name="seed">
        /// This feature is in Beta.
        /// If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.
        /// Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.
        /// </param>
        /// <param name="stop"> Up to 4 sequences where the API will stop generating further tokens. </param>
        /// <param name="stream"> If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions). </param>
        /// <param name="streamOptions"></param>
        /// <param name="temperature">
        /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
        ///
        /// We generally recommend altering this or `top_p` but not both.
        /// </param>
        /// <param name="topP">
        /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
        ///
        /// We generally recommend altering this or `temperature` but not both.
        /// </param>
        /// <param name="tools"> A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported. </param>
        /// <param name="toolChoice"></param>
        /// <param name="user"> A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids). </param>
        /// <param name="functionCall">
        /// Deprecated in favor of `tool_choice`.
        ///
        /// Controls which (if any) function is called by the model.
        /// `none` means the model will not call a function and instead generates a message.
        /// `auto` means the model can pick between generating a message or calling a function.
        /// Specifying a particular function via `{"name": "my_function"}` forces the model to call that function.
        ///
        /// `none` is the default when no functions are present. `auto` is the default if functions are present.
        /// </param>
        /// <param name="functions">
        /// Deprecated in favor of `tools`.
        ///
        /// A list of functions the model may generate JSON inputs for.
        /// </param>
        /// <param name="dataSources">
        /// Please note <see cref="AzureChatDataSource"/> is the base class. According to the scenario, a derived class of the base class might need to be assigned here, or this property needs to be casted to one of the possible derived classes.
        /// The available derived classes include <see cref="OpenAI.AzureChatMachineLearningIndexDataSource"/>, <see cref="OpenAI.AzureChatSearchDataSource"/>, <see cref="OpenAI.AzureChatCosmosDBDataSource"/>, <see cref="OpenAI.AzureChatElasticsearchDataSource"/> and <see cref="OpenAI.AzureChatPineconeDataSource"/>.
        /// </param>
        /// <returns> A new <see cref="OpenAI.AzureCreateChatCompletionRequest"/> instance for mocking. </returns>
        public static AzureCreateChatCompletionRequest AzureCreateChatCompletionRequest(IEnumerable<BinaryData> messages = null, string model = null, float? frequencyPenalty = null, IReadOnlyDictionary<string, int> logitBias = null, bool? logprobs = null, int? topLogprobs = null, int? maxTokens = null, int? n = null, float? presencePenalty = null, CreateChatCompletionRequestResponseFormat responseFormat = null, long? seed = null, BinaryData stop = null, bool? stream = null, ChatCompletionStreamOptions streamOptions = null, float? temperature = null, float? topP = null, IEnumerable<ChatCompletionTool> tools = null, BinaryData toolChoice = null, string user = null, BinaryData functionCall = null, IEnumerable<ChatCompletionFunctions> functions = null, IEnumerable<AzureChatDataSource> dataSources = null)
        {
            messages ??= new List<BinaryData>();
            logitBias ??= new Dictionary<string, int>();
            tools ??= new List<ChatCompletionTool>();
            functions ??= new List<ChatCompletionFunctions>();
            dataSources ??= new List<AzureChatDataSource>();

            return new AzureCreateChatCompletionRequest(
                messages?.ToList(),
                model,
                frequencyPenalty,
                logitBias,
                logprobs,
                topLogprobs,
                maxTokens,
                n,
                presencePenalty,
                responseFormat,
                seed,
                stop,
                stream,
                streamOptions,
                temperature,
                topP,
                tools?.ToList(),
                toolChoice,
                user,
                functionCall,
                functions?.ToList(),
                serializedAdditionalRawData: null,
                dataSources?.ToList());
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.CreateChatCompletionResponse"/>. </summary>
        /// <param name="id"> A unique identifier for the chat completion. </param>
        /// <param name="choices"> A list of chat completion choices. Can be more than one if `n` is greater than 1. </param>
        /// <param name="created"> The Unix timestamp (in seconds) of when the chat completion was created. </param>
        /// <param name="model"> The model used for the chat completion. </param>
        /// <param name="systemFingerprint">
        /// This fingerprint represents the backend configuration that the model runs with.
        ///
        /// Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.
        /// </param>
        /// <param name="object"> The object type, which is always `chat.completion`. </param>
        /// <param name="usage"></param>
        /// <returns> A new <see cref="OpenAI.CreateChatCompletionResponse"/> instance for mocking. </returns>
        public static CreateChatCompletionResponse CreateChatCompletionResponse(string id = null, IEnumerable<CreateChatCompletionResponseChoice> choices = null, DateTimeOffset created = default, string model = null, string systemFingerprint = null, string @object = null, CompletionUsage usage = null)
        {
            choices ??= new List<CreateChatCompletionResponseChoice>();

            return new CreateChatCompletionResponse(
                id,
                choices?.ToList(),
                created,
                model,
                systemFingerprint,
                @object,
                usage,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.CreateChatCompletionResponseChoice"/>. </summary>
        /// <param name="finishReason">
        /// The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence,
        /// `length` if the maximum number of tokens specified in the request was reached,
        /// `content_filter` if content was omitted due to a flag from our content filters,
        /// `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model called a function.
        /// </param>
        /// <param name="index"> The index of the choice in the list of choices. </param>
        /// <param name="message"></param>
        /// <param name="logprobs"> Log probability information for the choice. </param>
        /// <returns> A new <see cref="OpenAI.CreateChatCompletionResponseChoice"/> instance for mocking. </returns>
        public static CreateChatCompletionResponseChoice CreateChatCompletionResponseChoice(string finishReason = null, int index = default, ChatCompletionResponseMessage message = null, CreateChatCompletionResponseChoiceLogprobs logprobs = null)
        {
            return new CreateChatCompletionResponseChoice(finishReason, index, message, logprobs, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionResponseMessage"/>. </summary>
        /// <param name="content"> The contents of the message. </param>
        /// <param name="toolCalls"></param>
        /// <param name="role"> The role of the author of this message. </param>
        /// <param name="functionCall"> Deprecated and replaced by `tool_calls`. The name and arguments of a function that should be called, as generated by the model. </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionResponseMessage"/> instance for mocking. </returns>
        public static ChatCompletionResponseMessage ChatCompletionResponseMessage(string content = null, IEnumerable<ChatCompletionMessageToolCall> toolCalls = null, string role = null, ChatCompletionResponseMessageFunctionCall functionCall = null)
        {
            toolCalls ??= new List<ChatCompletionMessageToolCall>();

            return new ChatCompletionResponseMessage(content, toolCalls?.ToList(), role, functionCall, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionResponseMessageFunctionCall"/>. </summary>
        /// <param name="arguments"> The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function. </param>
        /// <param name="name"> The name of the function to call. </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionResponseMessageFunctionCall"/> instance for mocking. </returns>
        public static ChatCompletionResponseMessageFunctionCall ChatCompletionResponseMessageFunctionCall(string arguments = null, string name = null)
        {
            return new ChatCompletionResponseMessageFunctionCall(arguments, name, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.CreateChatCompletionResponseChoiceLogprobs"/>. </summary>
        /// <param name="content"> A list of message content tokens with log probability information. </param>
        /// <returns> A new <see cref="OpenAI.CreateChatCompletionResponseChoiceLogprobs"/> instance for mocking. </returns>
        public static CreateChatCompletionResponseChoiceLogprobs CreateChatCompletionResponseChoiceLogprobs(IEnumerable<ChatCompletionTokenLogprob> content = null)
        {
            content ??= new List<ChatCompletionTokenLogprob>();

            return new CreateChatCompletionResponseChoiceLogprobs(content?.ToList(), serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionTokenLogprob"/>. </summary>
        /// <param name="token"> The token. </param>
        /// <param name="logprob"> The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value `-9999.0` is used to signify that the token is very unlikely. </param>
        /// <param name="bytes"> A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be `null` if there is no bytes representation for the token. </param>
        /// <param name="topLogprobs"> List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested `top_logprobs` returned. </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionTokenLogprob"/> instance for mocking. </returns>
        public static ChatCompletionTokenLogprob ChatCompletionTokenLogprob(string token = null, float logprob = default, IEnumerable<int> bytes = null, IEnumerable<ChatCompletionTokenLogprobTopLogprob> topLogprobs = null)
        {
            bytes ??= new List<int>();
            topLogprobs ??= new List<ChatCompletionTokenLogprobTopLogprob>();

            return new ChatCompletionTokenLogprob(token, logprob, bytes?.ToList(), topLogprobs?.ToList(), serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ChatCompletionTokenLogprobTopLogprob"/>. </summary>
        /// <param name="token"> The token. </param>
        /// <param name="logprob"> The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value `-9999.0` is used to signify that the token is very unlikely. </param>
        /// <param name="bytes"> A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be `null` if there is no bytes representation for the token. </param>
        /// <returns> A new <see cref="OpenAI.ChatCompletionTokenLogprobTopLogprob"/> instance for mocking. </returns>
        public static ChatCompletionTokenLogprobTopLogprob ChatCompletionTokenLogprobTopLogprob(string token = null, float logprob = default, IEnumerable<int> bytes = null)
        {
            bytes ??= new List<int>();

            return new ChatCompletionTokenLogprobTopLogprob(token, logprob, bytes?.ToList(), serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.CompletionUsage"/>. </summary>
        /// <param name="completionTokens"> Number of tokens in the generated completion. </param>
        /// <param name="promptTokens"> Number of tokens in the prompt. </param>
        /// <param name="totalTokens"> Total number of tokens used in the request (prompt + completion). </param>
        /// <returns> A new <see cref="OpenAI.CompletionUsage"/> instance for mocking. </returns>
        public static CompletionUsage CompletionUsage(int completionTokens = default, int promptTokens = default, int totalTokens = default)
        {
            return new CompletionUsage(completionTokens, promptTokens, totalTokens, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureCreateChatCompletionResponse"/>. </summary>
        /// <param name="id"> A unique identifier for the chat completion. </param>
        /// <param name="choices"> A list of chat completion choices. Can be more than one if `n` is greater than 1. </param>
        /// <param name="created"> The Unix timestamp (in seconds) of when the chat completion was created. </param>
        /// <param name="model"> The model used for the chat completion. </param>
        /// <param name="systemFingerprint">
        /// This fingerprint represents the backend configuration that the model runs with.
        ///
        /// Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.
        /// </param>
        /// <param name="object"> The object type, which is always `chat.completion`. </param>
        /// <param name="usage"></param>
        /// <param name="promptFilterResults"></param>
        /// <returns> A new <see cref="OpenAI.AzureCreateChatCompletionResponse"/> instance for mocking. </returns>
        public static AzureCreateChatCompletionResponse AzureCreateChatCompletionResponse(string id = null, IEnumerable<CreateChatCompletionResponseChoice> choices = null, DateTimeOffset created = default, string model = null, string systemFingerprint = null, string @object = null, CompletionUsage usage = null, IEnumerable<AzureCreateChatCompletionResponsePromptFilterResult> promptFilterResults = null)
        {
            choices ??= new List<CreateChatCompletionResponseChoice>();
            promptFilterResults ??= new List<AzureCreateChatCompletionResponsePromptFilterResult>();

            return new AzureCreateChatCompletionResponse(
                id,
                choices?.ToList(),
                created,
                model,
                systemFingerprint,
                @object,
                usage,
                serializedAdditionalRawData: null,
                promptFilterResults?.ToList());
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureCreateChatCompletionResponsePromptFilterResult"/>. </summary>
        /// <param name="promptIndex"></param>
        /// <param name="contentFilterResults"></param>
        /// <returns> A new <see cref="OpenAI.AzureCreateChatCompletionResponsePromptFilterResult"/> instance for mocking. </returns>
        public static AzureCreateChatCompletionResponsePromptFilterResult AzureCreateChatCompletionResponsePromptFilterResult(int promptIndex = default, AzureContentFilterResultForPrompt contentFilterResults = null)
        {
            return new AzureCreateChatCompletionResponsePromptFilterResult(promptIndex, contentFilterResults, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureContentFilterResultForPrompt"/>. </summary>
        /// <param name="promptIndex"></param>
        /// <param name="contentFilterResults"></param>
        /// <returns> A new <see cref="OpenAI.AzureContentFilterResultForPrompt"/> instance for mocking. </returns>
        public static AzureContentFilterResultForPrompt AzureContentFilterResultForPrompt(int? promptIndex = null, AzureContentFilterResultForPromptContentFilterResults contentFilterResults = null)
        {
            return new AzureContentFilterResultForPrompt(promptIndex, contentFilterResults, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureContentFilterResultForPromptContentFilterResults"/>. </summary>
        /// <param name="sexual"></param>
        /// <param name="violence"></param>
        /// <param name="hate"></param>
        /// <param name="selfHarm"></param>
        /// <param name="profanity"></param>
        /// <param name="customBlocklists"></param>
        /// <param name="error"></param>
        /// <param name="jailbreak"></param>
        /// <param name="indirectAttack"></param>
        /// <returns> A new <see cref="OpenAI.AzureContentFilterResultForPromptContentFilterResults"/> instance for mocking. </returns>
        public static AzureContentFilterResultForPromptContentFilterResults AzureContentFilterResultForPromptContentFilterResults(AzureContentFilterSeverityResult sexual = null, AzureContentFilterSeverityResult violence = null, AzureContentFilterSeverityResult hate = null, AzureContentFilterSeverityResult selfHarm = null, AzureContentFilterDetectionResult profanity = null, AzureContentFilterBlocklistResult customBlocklists = null, AzureContentFilterResultForPromptContentFilterResultsError error = null, AzureContentFilterDetectionResult jailbreak = null, AzureContentFilterDetectionResult indirectAttack = null)
        {
            return new AzureContentFilterResultForPromptContentFilterResults(
                sexual,
                violence,
                hate,
                selfHarm,
                profanity,
                customBlocklists,
                error,
                jailbreak,
                indirectAttack,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureContentFilterSeverityResult"/>. </summary>
        /// <param name="filtered"></param>
        /// <param name="severity"></param>
        /// <returns> A new <see cref="OpenAI.AzureContentFilterSeverityResult"/> instance for mocking. </returns>
        public static AzureContentFilterSeverityResult AzureContentFilterSeverityResult(bool filtered = default, string severity = null)
        {
            return new AzureContentFilterSeverityResult(filtered, severity, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureContentFilterDetectionResult"/>. </summary>
        /// <param name="filtered"></param>
        /// <param name="detected"></param>
        /// <returns> A new <see cref="OpenAI.AzureContentFilterDetectionResult"/> instance for mocking. </returns>
        public static AzureContentFilterDetectionResult AzureContentFilterDetectionResult(bool filtered = default, bool detected = default)
        {
            return new AzureContentFilterDetectionResult(filtered, detected, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureContentFilterBlocklistResult"/>. </summary>
        /// <param name="filtered"></param>
        /// <param name="details"></param>
        /// <returns> A new <see cref="OpenAI.AzureContentFilterBlocklistResult"/> instance for mocking. </returns>
        public static AzureContentFilterBlocklistResult AzureContentFilterBlocklistResult(bool filtered = default, IEnumerable<AzureContentFilterBlocklistResultDetail> details = null)
        {
            details ??= new List<AzureContentFilterBlocklistResultDetail>();

            return new AzureContentFilterBlocklistResult(filtered, details?.ToList(), serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureContentFilterBlocklistResultDetail"/>. </summary>
        /// <param name="filtered"></param>
        /// <param name="id"></param>
        /// <returns> A new <see cref="OpenAI.AzureContentFilterBlocklistResultDetail"/> instance for mocking. </returns>
        public static AzureContentFilterBlocklistResultDetail AzureContentFilterBlocklistResultDetail(bool filtered = default, string id = null)
        {
            return new AzureContentFilterBlocklistResultDetail(filtered, id, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureContentFilterResultForPromptContentFilterResultsError"/>. </summary>
        /// <param name="code"></param>
        /// <param name="message"></param>
        /// <returns> A new <see cref="OpenAI.AzureContentFilterResultForPromptContentFilterResultsError"/> instance for mocking. </returns>
        public static AzureContentFilterResultForPromptContentFilterResultsError AzureContentFilterResultForPromptContentFilterResultsError(string code = null, string message = null)
        {
            return new AzureContentFilterResultForPromptContentFilterResultsError(code, message, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.CreateImageRequest"/>. </summary>
        /// <param name="prompt"> A text description of the desired image(s). The maximum length is 1000 characters for `dall-e-2` and 4000 characters for `dall-e-3`. </param>
        /// <param name="model"> The model to use for image generation. </param>
        /// <param name="n"> The number of images to generate. Must be between 1 and 10. For `dall-e-3`, only `n=1` is supported. </param>
        /// <param name="quality"> The quality of the image that will be generated. `hd` creates images with finer details and greater consistency across the image. This param is only supported for `dall-e-3`. </param>
        /// <param name="responseFormat"> The format in which the generated images are returned. Must be one of `url` or `b64_json`. URLs are only valid for 60 minutes after the image has been generated. </param>
        /// <param name="size"> The size of the generated images. Must be one of `256x256`, `512x512`, or `1024x1024` for `dall-e-2`. Must be one of `1024x1024`, `1792x1024`, or `1024x1792` for `dall-e-3` models. </param>
        /// <param name="style"> The style of the generated images. Must be one of `vivid` or `natural`. Vivid causes the model to lean towards generating hyper-real and dramatic images. Natural causes the model to produce more natural, less hyper-real looking images. This param is only supported for `dall-e-3`. </param>
        /// <param name="user"> A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids). </param>
        /// <returns> A new <see cref="OpenAI.CreateImageRequest"/> instance for mocking. </returns>
        public static CreateImageRequest CreateImageRequest(string prompt = null, string model = null, int? n = null, string quality = null, string responseFormat = null, string size = null, string style = null, string user = null)
        {
            return new CreateImageRequest(
                prompt,
                model,
                n,
                quality,
                responseFormat,
                size,
                style,
                user,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.ImagesResponse"/>. </summary>
        /// <param name="created"></param>
        /// <param name="data"></param>
        /// <returns> A new <see cref="OpenAI.ImagesResponse"/> instance for mocking. </returns>
        public static ImagesResponse ImagesResponse(DateTimeOffset created = default, IEnumerable<Image> data = null)
        {
            data ??= new List<Image>();

            return new ImagesResponse(created, data?.ToList(), serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.Image"/>. </summary>
        /// <param name="b64Json"> The base64-encoded JSON of the generated image, if `response_format` is `b64_json`. </param>
        /// <param name="url"> The URL of the generated image, if `response_format` is `url` (default). </param>
        /// <param name="revisedPrompt"> The prompt that was used to generate the image, if there was any revision to the prompt. </param>
        /// <returns> A new <see cref="OpenAI.Image"/> instance for mocking. </returns>
        public static Image Image(BinaryData b64Json = null, Uri url = null, string revisedPrompt = null)
        {
            return new Image(b64Json, url, revisedPrompt, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatSearchDataSource"/>. </summary>
        /// <param name="parameters"> The parameter information to control the use of the Azure Search data source. </param>
        /// <returns> A new <see cref="OpenAI.AzureChatSearchDataSource"/> instance for mocking. </returns>
        public static AzureChatSearchDataSource AzureChatSearchDataSource(AzureChatSearchDataSourceParameters parameters = null)
        {
            return new AzureChatSearchDataSource("azure_search", serializedAdditionalRawData: null, parameters);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatSearchDataSourceParameters"/>. </summary>
        /// <param name="authentication">
        /// The authentication mechanism to use with the data source.
        /// Please note <see cref="AzureChatDataSourceAuthenticationOptions"/> is the base class. According to the scenario, a derived class of the base class might need to be assigned here, or this property needs to be casted to one of the possible derived classes.
        /// The available derived classes include <see cref="OpenAI.AzureChatDataSourceAccessTokenAuthenticationOptions"/>, <see cref="OpenAI.AzureChatDataSourceApiKeyAuthenticationOptions"/>, <see cref="OpenAI.AzureChatDataSourceConnectionStringAuthenticationOptions"/>, <see cref="OpenAI.AzureChatDataSourceEncodedApiKeyAuthenticationOptions"/>, <see cref="OpenAI.AzureChatDataSourceKeyAndKeyIdAuthenticationOptions"/>, <see cref="AzureChatDataSourceSystemAssignedManagedIdentityAuthenticationOptions"/> and <see cref="OpenAI.AzureChatDataSourceUserAssignedManagedIdentityAuthenticationOptions"/>.
        /// </param>
        /// <param name="topNDocuments"> The configured number of documents to feature in the query. </param>
        /// <param name="inScope"> Whether queries should be restricted to use of the indexed data. </param>
        /// <param name="strictness">
        /// The configured strictness of the search relevance filtering.
        /// Higher strictness will increase precision but lower recall of the answer.
        /// </param>
        /// <param name="roleInformation">
        /// Additional instructions for the model to inform how it should behave and any context it should reference when
        /// generating a response. You can describe the assistant's personality and tell it how to format responses.
        /// This is limited to 100 tokens and counts against the overall token limit.
        /// </param>
        /// <param name="maxSearchQueries">
        /// The maximum number of rewritten queries that should be sent to the search provider for a single user message.
        /// By default, the system will make an automatic determination.
        /// </param>
        /// <param name="allowPartialResult">
        /// If set to true, the system will allow partial search results to be used and the request will fail if all
        /// partial queries fail. If not specified or specified as false, the request will fail if any search query fails.
        /// </param>
        /// <param name="includeContexts">
        /// The output context properties to include on the response.
        /// By default, citations and intent will be requested.
        /// </param>
        /// <param name="endpoint"> The absolute endpoint path for the Azure Search resource to use. </param>
        /// <param name="indexName"> The name of the index to use, as specified in the Azure Search resource. </param>
        /// <param name="fieldsMapping"> The field mappings to use with the Azure Search resource. </param>
        /// <param name="queryType"> The query type for the Azure Search resource to use. </param>
        /// <param name="semanticConfiguration"> Additional semantic configuration for the query. </param>
        /// <param name="filter"> A filter to apply to the search. </param>
        /// <param name="embeddingDependency">
        /// Please note <see cref="AzureChatDataSourceVectorizationSource"/> is the base class. According to the scenario, a derived class of the base class might need to be assigned here, or this property needs to be casted to one of the possible derived classes.
        /// The available derived classes include <see cref="OpenAI.AzureChatDataSourceDeploymentNameVectorizationSource"/>, <see cref="OpenAI.AzureChatDataSourceEndpointVectorizationSource"/> and <see cref="OpenAI.AzureChatDataSourceModelIdVectorizationSource"/>.
        /// </param>
        /// <returns> A new <see cref="OpenAI.AzureChatSearchDataSourceParameters"/> instance for mocking. </returns>
        public static AzureChatSearchDataSourceParameters AzureChatSearchDataSourceParameters(AzureChatDataSourceAuthenticationOptions authentication = null, int? topNDocuments = null, bool? inScope = null, int? strictness = null, string roleInformation = null, int? maxSearchQueries = null, bool allowPartialResult = default, IEnumerable<BinaryData> includeContexts = null, Uri endpoint = null, string indexName = null, AzureChatSearchDataSourceParametersFieldsMapping fieldsMapping = null, string queryType = null, string semanticConfiguration = null, string filter = null, AzureChatDataSourceVectorizationSource embeddingDependency = null)
        {
            includeContexts ??= new List<BinaryData>();

            return new AzureChatSearchDataSourceParameters(
                authentication,
                topNDocuments,
                inScope,
                strictness,
                roleInformation,
                maxSearchQueries,
                allowPartialResult,
                includeContexts?.ToList(),
                endpoint,
                indexName,
                fieldsMapping,
                queryType,
                semanticConfiguration,
                filter,
                embeddingDependency,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatSearchDataSourceParametersFieldsMapping"/>. </summary>
        /// <param name="titleField"> The name of the index field to use as a title. </param>
        /// <param name="urlField"> The name of the index field to use as a URL. </param>
        /// <param name="filepathField"> The name of the index field to use as a filepath. </param>
        /// <param name="contentFields"> The names of index fields that should be treated as content. </param>
        /// <param name="contentFieldsSeparator"> The separator pattern that content fields should use. </param>
        /// <param name="vectorFields"> THe names of fields that represent vector data. </param>
        /// <param name="imageVectorFields"> The names of fields that represent image vector data. </param>
        /// <returns> A new <see cref="OpenAI.AzureChatSearchDataSourceParametersFieldsMapping"/> instance for mocking. </returns>
        public static AzureChatSearchDataSourceParametersFieldsMapping AzureChatSearchDataSourceParametersFieldsMapping(string titleField = null, string urlField = null, string filepathField = null, IEnumerable<string> contentFields = null, string contentFieldsSeparator = null, IEnumerable<string> vectorFields = null, IEnumerable<string> imageVectorFields = null)
        {
            contentFields ??= new List<string>();
            vectorFields ??= new List<string>();
            imageVectorFields ??= new List<string>();

            return new AzureChatSearchDataSourceParametersFieldsMapping(
                titleField,
                urlField,
                filepathField,
                contentFields?.ToList(),
                contentFieldsSeparator,
                vectorFields?.ToList(),
                imageVectorFields?.ToList(),
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatMachineLearningIndexDataSource"/>. </summary>
        /// <param name="parameters"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatMachineLearningIndexDataSource"/> instance for mocking. </returns>
        public static AzureChatMachineLearningIndexDataSource AzureChatMachineLearningIndexDataSource(AzureChatMachineLearningIndexDataSourceParameters parameters = null)
        {
            return new AzureChatMachineLearningIndexDataSource("azure_ml_index", serializedAdditionalRawData: null, parameters);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatMachineLearningIndexDataSourceParameters"/>. </summary>
        /// <param name="authentication">
        /// The authentication mechanism to use with the data source.
        /// Please note <see cref="AzureChatDataSourceAuthenticationOptions"/> is the base class. According to the scenario, a derived class of the base class might need to be assigned here, or this property needs to be casted to one of the possible derived classes.
        /// The available derived classes include <see cref="OpenAI.AzureChatDataSourceAccessTokenAuthenticationOptions"/>, <see cref="OpenAI.AzureChatDataSourceApiKeyAuthenticationOptions"/>, <see cref="OpenAI.AzureChatDataSourceConnectionStringAuthenticationOptions"/>, <see cref="OpenAI.AzureChatDataSourceEncodedApiKeyAuthenticationOptions"/>, <see cref="OpenAI.AzureChatDataSourceKeyAndKeyIdAuthenticationOptions"/>, <see cref="AzureChatDataSourceSystemAssignedManagedIdentityAuthenticationOptions"/> and <see cref="OpenAI.AzureChatDataSourceUserAssignedManagedIdentityAuthenticationOptions"/>.
        /// </param>
        /// <param name="topNDocuments"> The configured number of documents to feature in the query. </param>
        /// <param name="inScope"> Whether queries should be restricted to use of the indexed data. </param>
        /// <param name="strictness">
        /// The configured strictness of the search relevance filtering.
        /// Higher strictness will increase precision but lower recall of the answer.
        /// </param>
        /// <param name="roleInformation">
        /// Additional instructions for the model to inform how it should behave and any context it should reference when
        /// generating a response. You can describe the assistant's personality and tell it how to format responses.
        /// This is limited to 100 tokens and counts against the overall token limit.
        /// </param>
        /// <param name="maxSearchQueries">
        /// The maximum number of rewritten queries that should be sent to the search provider for a single user message.
        /// By default, the system will make an automatic determination.
        /// </param>
        /// <param name="allowPartialResult">
        /// If set to true, the system will allow partial search results to be used and the request will fail if all
        /// partial queries fail. If not specified or specified as false, the request will fail if any search query fails.
        /// </param>
        /// <param name="includeContexts">
        /// The output context properties to include on the response.
        /// By default, citations and intent will be requested.
        /// </param>
        /// <param name="projectResourceId"> The ID of the Azure Machine Learning index project to use. </param>
        /// <param name="name"> The name of the Azure Machine Learning index to use. </param>
        /// <param name="version"> The version of the vector index to use. </param>
        /// <param name="filter"> A search filter, which is only applicable if the vector index is of the 'AzureSearch' type. </param>
        /// <returns> A new <see cref="OpenAI.AzureChatMachineLearningIndexDataSourceParameters"/> instance for mocking. </returns>
        public static AzureChatMachineLearningIndexDataSourceParameters AzureChatMachineLearningIndexDataSourceParameters(AzureChatDataSourceAuthenticationOptions authentication = null, int? topNDocuments = null, bool? inScope = null, int? strictness = null, string roleInformation = null, int? maxSearchQueries = null, bool allowPartialResult = default, IEnumerable<BinaryData> includeContexts = null, string projectResourceId = null, string name = null, string version = null, string filter = null)
        {
            includeContexts ??= new List<BinaryData>();

            return new AzureChatMachineLearningIndexDataSourceParameters(
                authentication,
                topNDocuments,
                inScope,
                strictness,
                roleInformation,
                maxSearchQueries,
                allowPartialResult,
                includeContexts?.ToList(),
                projectResourceId,
                name,
                version,
                filter,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatCosmosDBDataSource"/>. </summary>
        /// <param name="parameters"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatCosmosDBDataSource"/> instance for mocking. </returns>
        public static AzureChatCosmosDBDataSource AzureChatCosmosDBDataSource(AzureChatCosmosDBDataSourceParameters parameters = null)
        {
            return new AzureChatCosmosDBDataSource("AzureCosmosDB", serializedAdditionalRawData: null, parameters);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatCosmosDBDataSourceParameters"/>. </summary>
        /// <param name="authentication">
        /// The authentication mechanism to use with the data source.
        /// Please note <see cref="AzureChatDataSourceAuthenticationOptions"/> is the base class. According to the scenario, a derived class of the base class might need to be assigned here, or this property needs to be casted to one of the possible derived classes.
        /// The available derived classes include <see cref="OpenAI.AzureChatDataSourceAccessTokenAuthenticationOptions"/>, <see cref="OpenAI.AzureChatDataSourceApiKeyAuthenticationOptions"/>, <see cref="OpenAI.AzureChatDataSourceConnectionStringAuthenticationOptions"/>, <see cref="OpenAI.AzureChatDataSourceEncodedApiKeyAuthenticationOptions"/>, <see cref="OpenAI.AzureChatDataSourceKeyAndKeyIdAuthenticationOptions"/>, <see cref="AzureChatDataSourceSystemAssignedManagedIdentityAuthenticationOptions"/> and <see cref="OpenAI.AzureChatDataSourceUserAssignedManagedIdentityAuthenticationOptions"/>.
        /// </param>
        /// <param name="topNDocuments"> The configured number of documents to feature in the query. </param>
        /// <param name="inScope"> Whether queries should be restricted to use of the indexed data. </param>
        /// <param name="strictness">
        /// The configured strictness of the search relevance filtering.
        /// Higher strictness will increase precision but lower recall of the answer.
        /// </param>
        /// <param name="roleInformation">
        /// Additional instructions for the model to inform how it should behave and any context it should reference when
        /// generating a response. You can describe the assistant's personality and tell it how to format responses.
        /// This is limited to 100 tokens and counts against the overall token limit.
        /// </param>
        /// <param name="maxSearchQueries">
        /// The maximum number of rewritten queries that should be sent to the search provider for a single user message.
        /// By default, the system will make an automatic determination.
        /// </param>
        /// <param name="allowPartialResult">
        /// If set to true, the system will allow partial search results to be used and the request will fail if all
        /// partial queries fail. If not specified or specified as false, the request will fail if any search query fails.
        /// </param>
        /// <param name="includeContexts">
        /// The output context properties to include on the response.
        /// By default, citations and intent will be requested.
        /// </param>
        /// <param name="containerName"></param>
        /// <param name="databaseName"></param>
        /// <param name="embeddingDependency">
        /// Please note <see cref="AzureChatDataSourceVectorizationSource"/> is the base class. According to the scenario, a derived class of the base class might need to be assigned here, or this property needs to be casted to one of the possible derived classes.
        /// The available derived classes include <see cref="OpenAI.AzureChatDataSourceDeploymentNameVectorizationSource"/>, <see cref="OpenAI.AzureChatDataSourceEndpointVectorizationSource"/> and <see cref="OpenAI.AzureChatDataSourceModelIdVectorizationSource"/>.
        /// </param>
        /// <param name="indexName"></param>
        /// <param name="fieldsMapping"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatCosmosDBDataSourceParameters"/> instance for mocking. </returns>
        public static AzureChatCosmosDBDataSourceParameters AzureChatCosmosDBDataSourceParameters(AzureChatDataSourceAuthenticationOptions authentication = null, int? topNDocuments = null, bool? inScope = null, int? strictness = null, string roleInformation = null, int? maxSearchQueries = null, bool allowPartialResult = default, IEnumerable<BinaryData> includeContexts = null, string containerName = null, string databaseName = null, AzureChatDataSourceVectorizationSource embeddingDependency = null, string indexName = null, AzureChatCosmosDBDataSourceParametersFieldsMapping fieldsMapping = null)
        {
            includeContexts ??= new List<BinaryData>();

            return new AzureChatCosmosDBDataSourceParameters(
                authentication,
                topNDocuments,
                inScope,
                strictness,
                roleInformation,
                maxSearchQueries,
                allowPartialResult,
                includeContexts?.ToList(),
                containerName,
                databaseName,
                embeddingDependency,
                indexName,
                fieldsMapping,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatCosmosDBDataSourceParametersFieldsMapping"/>. </summary>
        /// <param name="contentFields"></param>
        /// <param name="vectorFields"></param>
        /// <param name="titleField"></param>
        /// <param name="urlField"></param>
        /// <param name="filepathField"></param>
        /// <param name="contentFieldsSeparator"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatCosmosDBDataSourceParametersFieldsMapping"/> instance for mocking. </returns>
        public static AzureChatCosmosDBDataSourceParametersFieldsMapping AzureChatCosmosDBDataSourceParametersFieldsMapping(IEnumerable<string> contentFields = null, IEnumerable<string> vectorFields = null, string titleField = null, string urlField = null, string filepathField = null, string contentFieldsSeparator = null)
        {
            contentFields ??= new List<string>();
            vectorFields ??= new List<string>();

            return new AzureChatCosmosDBDataSourceParametersFieldsMapping(
                contentFields?.ToList(),
                vectorFields?.ToList(),
                titleField,
                urlField,
                filepathField,
                contentFieldsSeparator,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatElasticsearchDataSource"/>. </summary>
        /// <param name="parameters"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatElasticsearchDataSource"/> instance for mocking. </returns>
        public static AzureChatElasticsearchDataSource AzureChatElasticsearchDataSource(AzureChatElasticsearchDataSourceParameters parameters = null)
        {
            return new AzureChatElasticsearchDataSource("elasticsearch", serializedAdditionalRawData: null, parameters);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatElasticsearchDataSourceParameters"/>. </summary>
        /// <param name="authentication">
        /// The authentication mechanism to use with the data source.
        /// Please note <see cref="AzureChatDataSourceAuthenticationOptions"/> is the base class. According to the scenario, a derived class of the base class might need to be assigned here, or this property needs to be casted to one of the possible derived classes.
        /// The available derived classes include <see cref="OpenAI.AzureChatDataSourceAccessTokenAuthenticationOptions"/>, <see cref="OpenAI.AzureChatDataSourceApiKeyAuthenticationOptions"/>, <see cref="OpenAI.AzureChatDataSourceConnectionStringAuthenticationOptions"/>, <see cref="OpenAI.AzureChatDataSourceEncodedApiKeyAuthenticationOptions"/>, <see cref="OpenAI.AzureChatDataSourceKeyAndKeyIdAuthenticationOptions"/>, <see cref="AzureChatDataSourceSystemAssignedManagedIdentityAuthenticationOptions"/> and <see cref="OpenAI.AzureChatDataSourceUserAssignedManagedIdentityAuthenticationOptions"/>.
        /// </param>
        /// <param name="topNDocuments"> The configured number of documents to feature in the query. </param>
        /// <param name="inScope"> Whether queries should be restricted to use of the indexed data. </param>
        /// <param name="strictness">
        /// The configured strictness of the search relevance filtering.
        /// Higher strictness will increase precision but lower recall of the answer.
        /// </param>
        /// <param name="roleInformation">
        /// Additional instructions for the model to inform how it should behave and any context it should reference when
        /// generating a response. You can describe the assistant's personality and tell it how to format responses.
        /// This is limited to 100 tokens and counts against the overall token limit.
        /// </param>
        /// <param name="maxSearchQueries">
        /// The maximum number of rewritten queries that should be sent to the search provider for a single user message.
        /// By default, the system will make an automatic determination.
        /// </param>
        /// <param name="allowPartialResult">
        /// If set to true, the system will allow partial search results to be used and the request will fail if all
        /// partial queries fail. If not specified or specified as false, the request will fail if any search query fails.
        /// </param>
        /// <param name="includeContexts">
        /// The output context properties to include on the response.
        /// By default, citations and intent will be requested.
        /// </param>
        /// <param name="endpoint"></param>
        /// <param name="indexName"></param>
        /// <param name="fieldsMapping"></param>
        /// <param name="queryType"></param>
        /// <param name="embeddingDependency">
        /// Please note <see cref="AzureChatDataSourceVectorizationSource"/> is the base class. According to the scenario, a derived class of the base class might need to be assigned here, or this property needs to be casted to one of the possible derived classes.
        /// The available derived classes include <see cref="OpenAI.AzureChatDataSourceDeploymentNameVectorizationSource"/>, <see cref="OpenAI.AzureChatDataSourceEndpointVectorizationSource"/> and <see cref="OpenAI.AzureChatDataSourceModelIdVectorizationSource"/>.
        /// </param>
        /// <returns> A new <see cref="OpenAI.AzureChatElasticsearchDataSourceParameters"/> instance for mocking. </returns>
        public static AzureChatElasticsearchDataSourceParameters AzureChatElasticsearchDataSourceParameters(AzureChatDataSourceAuthenticationOptions authentication = null, int? topNDocuments = null, bool? inScope = null, int? strictness = null, string roleInformation = null, int? maxSearchQueries = null, bool allowPartialResult = default, IEnumerable<BinaryData> includeContexts = null, Uri endpoint = null, string indexName = null, AzureChatElasticsearchDataSourceParametersFieldsMapping fieldsMapping = null, string queryType = null, AzureChatDataSourceVectorizationSource embeddingDependency = null)
        {
            includeContexts ??= new List<BinaryData>();

            return new AzureChatElasticsearchDataSourceParameters(
                authentication,
                topNDocuments,
                inScope,
                strictness,
                roleInformation,
                maxSearchQueries,
                allowPartialResult,
                includeContexts?.ToList(),
                endpoint,
                indexName,
                fieldsMapping,
                queryType,
                embeddingDependency,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatElasticsearchDataSourceParametersFieldsMapping"/>. </summary>
        /// <param name="titleField"></param>
        /// <param name="urlField"></param>
        /// <param name="filepathField"></param>
        /// <param name="contentFields"></param>
        /// <param name="contentFieldsSeparator"></param>
        /// <param name="vectorFields"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatElasticsearchDataSourceParametersFieldsMapping"/> instance for mocking. </returns>
        public static AzureChatElasticsearchDataSourceParametersFieldsMapping AzureChatElasticsearchDataSourceParametersFieldsMapping(string titleField = null, string urlField = null, string filepathField = null, IEnumerable<string> contentFields = null, string contentFieldsSeparator = null, IEnumerable<string> vectorFields = null)
        {
            contentFields ??= new List<string>();
            vectorFields ??= new List<string>();

            return new AzureChatElasticsearchDataSourceParametersFieldsMapping(
                titleField,
                urlField,
                filepathField,
                contentFields?.ToList(),
                contentFieldsSeparator,
                vectorFields?.ToList(),
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatPineconeDataSource"/>. </summary>
        /// <param name="authentication">
        /// The authentication mechanism to use with the data source.
        /// Please note <see cref="AzureChatDataSourceAuthenticationOptions"/> is the base class. According to the scenario, a derived class of the base class might need to be assigned here, or this property needs to be casted to one of the possible derived classes.
        /// The available derived classes include <see cref="OpenAI.AzureChatDataSourceAccessTokenAuthenticationOptions"/>, <see cref="OpenAI.AzureChatDataSourceApiKeyAuthenticationOptions"/>, <see cref="OpenAI.AzureChatDataSourceConnectionStringAuthenticationOptions"/>, <see cref="OpenAI.AzureChatDataSourceEncodedApiKeyAuthenticationOptions"/>, <see cref="OpenAI.AzureChatDataSourceKeyAndKeyIdAuthenticationOptions"/>, <see cref="AzureChatDataSourceSystemAssignedManagedIdentityAuthenticationOptions"/> and <see cref="OpenAI.AzureChatDataSourceUserAssignedManagedIdentityAuthenticationOptions"/>.
        /// </param>
        /// <param name="topNDocuments"> The configured number of documents to feature in the query. </param>
        /// <param name="inScope"> Whether queries should be restricted to use of the indexed data. </param>
        /// <param name="strictness">
        /// The configured strictness of the search relevance filtering.
        /// Higher strictness will increase precision but lower recall of the answer.
        /// </param>
        /// <param name="roleInformation">
        /// Additional instructions for the model to inform how it should behave and any context it should reference when
        /// generating a response. You can describe the assistant's personality and tell it how to format responses.
        /// This is limited to 100 tokens and counts against the overall token limit.
        /// </param>
        /// <param name="maxSearchQueries">
        /// The maximum number of rewritten queries that should be sent to the search provider for a single user message.
        /// By default, the system will make an automatic determination.
        /// </param>
        /// <param name="allowPartialResult">
        /// If set to true, the system will allow partial search results to be used and the request will fail if all
        /// partial queries fail. If not specified or specified as false, the request will fail if any search query fails.
        /// </param>
        /// <param name="includeContexts">
        /// The output context properties to include on the response.
        /// By default, citations and intent will be requested.
        /// </param>
        /// <param name="environment"></param>
        /// <param name="indexName"></param>
        /// <param name="embeddingDependency">
        /// Please note <see cref="AzureChatDataSourceVectorizationSource"/> is the base class. According to the scenario, a derived class of the base class might need to be assigned here, or this property needs to be casted to one of the possible derived classes.
        /// The available derived classes include <see cref="OpenAI.AzureChatDataSourceDeploymentNameVectorizationSource"/>, <see cref="OpenAI.AzureChatDataSourceEndpointVectorizationSource"/> and <see cref="OpenAI.AzureChatDataSourceModelIdVectorizationSource"/>.
        /// </param>
        /// <param name="fieldsMapping"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatPineconeDataSource"/> instance for mocking. </returns>
        public static AzureChatPineconeDataSource AzureChatPineconeDataSource(AzureChatDataSourceAuthenticationOptions authentication = null, int? topNDocuments = null, bool? inScope = null, int? strictness = null, string roleInformation = null, int? maxSearchQueries = null, bool allowPartialResult = default, IEnumerable<BinaryData> includeContexts = null, string environment = null, string indexName = null, AzureChatDataSourceVectorizationSource embeddingDependency = null, AzureChatPineconeDataSourceFieldsMapping fieldsMapping = null)
        {
            includeContexts ??= new List<BinaryData>();

            return new AzureChatPineconeDataSource(
                "pinecone",
                serializedAdditionalRawData: null,
                authentication,
                topNDocuments,
                inScope,
                strictness,
                roleInformation,
                maxSearchQueries,
                allowPartialResult,
                includeContexts?.ToList(),
                environment,
                indexName,
                embeddingDependency,
                fieldsMapping);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatPineconeDataSourceFieldsMapping"/>. </summary>
        /// <param name="contentFields"></param>
        /// <param name="titleField"></param>
        /// <param name="urlField"></param>
        /// <param name="filepathField"></param>
        /// <param name="contentFieldsSeparator"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatPineconeDataSourceFieldsMapping"/> instance for mocking. </returns>
        public static AzureChatPineconeDataSourceFieldsMapping AzureChatPineconeDataSourceFieldsMapping(IEnumerable<string> contentFields = null, string titleField = null, string urlField = null, string filepathField = null, string contentFieldsSeparator = null)
        {
            contentFields ??= new List<string>();

            return new AzureChatPineconeDataSourceFieldsMapping(
                contentFields?.ToList(),
                titleField,
                urlField,
                filepathField,
                contentFieldsSeparator,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatDataSourceApiKeyAuthenticationOptions"/>. </summary>
        /// <param name="key"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatDataSourceApiKeyAuthenticationOptions"/> instance for mocking. </returns>
        public static AzureChatDataSourceApiKeyAuthenticationOptions AzureChatDataSourceApiKeyAuthenticationOptions(string key = null)
        {
            return new AzureChatDataSourceApiKeyAuthenticationOptions("api_key", serializedAdditionalRawData: null, key);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatDataSourceConnectionStringAuthenticationOptions"/>. </summary>
        /// <param name="connectionString"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatDataSourceConnectionStringAuthenticationOptions"/> instance for mocking. </returns>
        public static AzureChatDataSourceConnectionStringAuthenticationOptions AzureChatDataSourceConnectionStringAuthenticationOptions(string connectionString = null)
        {
            return new AzureChatDataSourceConnectionStringAuthenticationOptions("connection_string", serializedAdditionalRawData: null, connectionString);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatDataSourceKeyAndKeyIdAuthenticationOptions"/>. </summary>
        /// <param name="key"></param>
        /// <param name="keyId"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatDataSourceKeyAndKeyIdAuthenticationOptions"/> instance for mocking. </returns>
        public static AzureChatDataSourceKeyAndKeyIdAuthenticationOptions AzureChatDataSourceKeyAndKeyIdAuthenticationOptions(string key = null, string keyId = null)
        {
            return new AzureChatDataSourceKeyAndKeyIdAuthenticationOptions("key_and_key_id", serializedAdditionalRawData: null, key, keyId);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatDataSourceEncodedApiKeyAuthenticationOptions"/>. </summary>
        /// <param name="encodedApiKey"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatDataSourceEncodedApiKeyAuthenticationOptions"/> instance for mocking. </returns>
        public static AzureChatDataSourceEncodedApiKeyAuthenticationOptions AzureChatDataSourceEncodedApiKeyAuthenticationOptions(string encodedApiKey = null)
        {
            return new AzureChatDataSourceEncodedApiKeyAuthenticationOptions("encoded_api_key", serializedAdditionalRawData: null, encodedApiKey);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatDataSourceAccessTokenAuthenticationOptions"/>. </summary>
        /// <param name="accessToken"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatDataSourceAccessTokenAuthenticationOptions"/> instance for mocking. </returns>
        public static AzureChatDataSourceAccessTokenAuthenticationOptions AzureChatDataSourceAccessTokenAuthenticationOptions(string accessToken = null)
        {
            return new AzureChatDataSourceAccessTokenAuthenticationOptions("access_token", serializedAdditionalRawData: null, accessToken);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatDataSourceUserAssignedManagedIdentityAuthenticationOptions"/>. </summary>
        /// <param name="managedIdentityResourceId"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatDataSourceUserAssignedManagedIdentityAuthenticationOptions"/> instance for mocking. </returns>
        public static AzureChatDataSourceUserAssignedManagedIdentityAuthenticationOptions AzureChatDataSourceUserAssignedManagedIdentityAuthenticationOptions(string managedIdentityResourceId = null)
        {
            return new AzureChatDataSourceUserAssignedManagedIdentityAuthenticationOptions("user_assigned_managed_identity", serializedAdditionalRawData: null, managedIdentityResourceId);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatDataSourceEndpointVectorizationSource"/>. </summary>
        /// <param name="endpoint"></param>
        /// <param name="authentication">
        /// Please note <see cref="AzureChatDataSourceAuthenticationOptions"/> is the base class. According to the scenario, a derived class of the base class might need to be assigned here, or this property needs to be casted to one of the possible derived classes.
        /// The available derived classes include <see cref="OpenAI.AzureChatDataSourceAccessTokenAuthenticationOptions"/>, <see cref="OpenAI.AzureChatDataSourceApiKeyAuthenticationOptions"/>, <see cref="OpenAI.AzureChatDataSourceConnectionStringAuthenticationOptions"/>, <see cref="OpenAI.AzureChatDataSourceEncodedApiKeyAuthenticationOptions"/>, <see cref="OpenAI.AzureChatDataSourceKeyAndKeyIdAuthenticationOptions"/>, <see cref="AzureChatDataSourceSystemAssignedManagedIdentityAuthenticationOptions"/> and <see cref="OpenAI.AzureChatDataSourceUserAssignedManagedIdentityAuthenticationOptions"/>.
        /// </param>
        /// <param name="dimensions"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatDataSourceEndpointVectorizationSource"/> instance for mocking. </returns>
        public static AzureChatDataSourceEndpointVectorizationSource AzureChatDataSourceEndpointVectorizationSource(Uri endpoint = null, AzureChatDataSourceAuthenticationOptions authentication = null, int? dimensions = null)
        {
            return new AzureChatDataSourceEndpointVectorizationSource("endpoint", serializedAdditionalRawData: null, endpoint, authentication, dimensions);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatDataSourceDeploymentNameVectorizationSource"/>. </summary>
        /// <param name="deploymentName"></param>
        /// <param name="dimensions"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatDataSourceDeploymentNameVectorizationSource"/> instance for mocking. </returns>
        public static AzureChatDataSourceDeploymentNameVectorizationSource AzureChatDataSourceDeploymentNameVectorizationSource(string deploymentName = null, int? dimensions = null)
        {
            return new AzureChatDataSourceDeploymentNameVectorizationSource("deployment_name", serializedAdditionalRawData: null, deploymentName, dimensions);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatDataSourceModelIdVectorizationSource"/>. </summary>
        /// <param name="modelId"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatDataSourceModelIdVectorizationSource"/> instance for mocking. </returns>
        public static AzureChatDataSourceModelIdVectorizationSource AzureChatDataSourceModelIdVectorizationSource(string modelId = null)
        {
            return new AzureChatDataSourceModelIdVectorizationSource("model_id", serializedAdditionalRawData: null, modelId);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatCompletionResponseMessage"/>. </summary>
        /// <param name="content"> The contents of the message. </param>
        /// <param name="toolCalls"></param>
        /// <param name="role"> The role of the author of this message. </param>
        /// <param name="functionCall"> Deprecated and replaced by `tool_calls`. The name and arguments of a function that should be called, as generated by the model. </param>
        /// <param name="context"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatCompletionResponseMessage"/> instance for mocking. </returns>
        public static AzureChatCompletionResponseMessage AzureChatCompletionResponseMessage(string content = null, IEnumerable<ChatCompletionMessageToolCall> toolCalls = null, string role = null, ChatCompletionResponseMessageFunctionCall functionCall = null, AzureChatCompletionResponseMessageContext context = null)
        {
            toolCalls ??= new List<ChatCompletionMessageToolCall>();

            return new AzureChatCompletionResponseMessage(
                content,
                toolCalls?.ToList(),
                role,
                functionCall,
                serializedAdditionalRawData: null,
                context);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatCompletionResponseMessageContext"/>. </summary>
        /// <param name="intent"></param>
        /// <param name="citations"></param>
        /// <param name="allRetrievedDocuments"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatCompletionResponseMessageContext"/> instance for mocking. </returns>
        public static AzureChatCompletionResponseMessageContext AzureChatCompletionResponseMessageContext(string intent = null, IEnumerable<AzureChatCompletionResponseMessageContextCitation> citations = null, IEnumerable<AzureChatCompletionResponseMessageContextAllRetrievedDocument> allRetrievedDocuments = null)
        {
            citations ??= new List<AzureChatCompletionResponseMessageContextCitation>();
            allRetrievedDocuments ??= new List<AzureChatCompletionResponseMessageContextAllRetrievedDocument>();

            return new AzureChatCompletionResponseMessageContext(intent, citations?.ToList(), allRetrievedDocuments?.ToList(), serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatCompletionResponseMessageContextCitation"/>. </summary>
        /// <param name="content"></param>
        /// <param name="title"></param>
        /// <param name="url"></param>
        /// <param name="filepath"></param>
        /// <param name="chunkId"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatCompletionResponseMessageContextCitation"/> instance for mocking. </returns>
        public static AzureChatCompletionResponseMessageContextCitation AzureChatCompletionResponseMessageContextCitation(string content = null, string title = null, string url = null, string filepath = null, string chunkId = null)
        {
            return new AzureChatCompletionResponseMessageContextCitation(
                content,
                title,
                url,
                filepath,
                chunkId,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureChatCompletionResponseMessageContextAllRetrievedDocument"/>. </summary>
        /// <param name="content"></param>
        /// <param name="title"></param>
        /// <param name="url"></param>
        /// <param name="filepath"></param>
        /// <param name="chunkId"></param>
        /// <param name="searchQueries"></param>
        /// <param name="dataSourceIndex"></param>
        /// <param name="originalSearchScore"></param>
        /// <param name="rerankScore"></param>
        /// <param name="filterReason"></param>
        /// <returns> A new <see cref="OpenAI.AzureChatCompletionResponseMessageContextAllRetrievedDocument"/> instance for mocking. </returns>
        public static AzureChatCompletionResponseMessageContextAllRetrievedDocument AzureChatCompletionResponseMessageContextAllRetrievedDocument(string content = null, string title = null, string url = null, string filepath = null, string chunkId = null, IEnumerable<string> searchQueries = null, int dataSourceIndex = default, double? originalSearchScore = null, double? rerankScore = null, string filterReason = null)
        {
            searchQueries ??= new List<string>();

            return new AzureChatCompletionResponseMessageContextAllRetrievedDocument(
                content,
                title,
                url,
                filepath,
                chunkId,
                searchQueries?.ToList(),
                dataSourceIndex,
                originalSearchScore,
                rerankScore,
                filterReason,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureContentFilterBlocklistIdResult"/>. </summary>
        /// <param name="id"></param>
        /// <param name="filtered"></param>
        /// <returns> A new <see cref="OpenAI.AzureContentFilterBlocklistIdResult"/> instance for mocking. </returns>
        public static AzureContentFilterBlocklistIdResult AzureContentFilterBlocklistIdResult(string id = null, bool filtered = default)
        {
            return new AzureContentFilterBlocklistIdResult(id, filtered, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureContentFilterResultForChoice"/>. </summary>
        /// <param name="sexual"></param>
        /// <param name="violence"></param>
        /// <param name="hate"></param>
        /// <param name="selfHarm"></param>
        /// <param name="profanity"></param>
        /// <param name="customBlocklists"></param>
        /// <param name="error"></param>
        /// <param name="protectedMaterialText"></param>
        /// <param name="protectedMaterialCode"></param>
        /// <returns> A new <see cref="OpenAI.AzureContentFilterResultForChoice"/> instance for mocking. </returns>
        public static AzureContentFilterResultForChoice AzureContentFilterResultForChoice(AzureContentFilterSeverityResult sexual = null, AzureContentFilterSeverityResult violence = null, AzureContentFilterSeverityResult hate = null, AzureContentFilterSeverityResult selfHarm = null, AzureContentFilterDetectionResult profanity = null, AzureContentFilterBlocklistResult customBlocklists = null, AzureContentFilterResultForPromptContentFilterResultsError error = null, AzureContentFilterDetectionResult protectedMaterialText = null, AzureContentFilterResultForChoiceProtectedMaterialCode protectedMaterialCode = null)
        {
            return new AzureContentFilterResultForChoice(
                sexual,
                violence,
                hate,
                selfHarm,
                profanity,
                customBlocklists,
                error,
                protectedMaterialText,
                protectedMaterialCode,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureContentFilterResultForChoiceProtectedMaterialCode"/>. </summary>
        /// <param name="filtered"></param>
        /// <param name="detected"></param>
        /// <param name="citation"></param>
        /// <returns> A new <see cref="OpenAI.AzureContentFilterResultForChoiceProtectedMaterialCode"/> instance for mocking. </returns>
        public static AzureContentFilterResultForChoiceProtectedMaterialCode AzureContentFilterResultForChoiceProtectedMaterialCode(bool filtered = default, bool detected = default, AzureContentFilterResultForChoiceProtectedMaterialCodeCitation citation = null)
        {
            return new AzureContentFilterResultForChoiceProtectedMaterialCode(filtered, detected, citation, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureContentFilterResultForChoiceProtectedMaterialCodeCitation"/>. </summary>
        /// <param name="license"></param>
        /// <param name="url"></param>
        /// <returns> A new <see cref="OpenAI.AzureContentFilterResultForChoiceProtectedMaterialCodeCitation"/> instance for mocking. </returns>
        public static AzureContentFilterResultForChoiceProtectedMaterialCodeCitation AzureContentFilterResultForChoiceProtectedMaterialCodeCitation(string license = null, Uri url = null)
        {
            return new AzureContentFilterResultForChoiceProtectedMaterialCodeCitation(license, url, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureContentFilterImageResponseResults"/>. </summary>
        /// <param name="sexual"></param>
        /// <param name="violence"></param>
        /// <param name="hate"></param>
        /// <param name="selfHarm"></param>
        /// <returns> A new <see cref="OpenAI.AzureContentFilterImageResponseResults"/> instance for mocking. </returns>
        public static AzureContentFilterImageResponseResults AzureContentFilterImageResponseResults(AzureContentFilterSeverityResult sexual = null, AzureContentFilterSeverityResult violence = null, AzureContentFilterSeverityResult hate = null, AzureContentFilterSeverityResult selfHarm = null)
        {
            return new AzureContentFilterImageResponseResults(sexual, violence, hate, selfHarm, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureContentFilterImagePromptResults"/>. </summary>
        /// <param name="sexual"></param>
        /// <param name="violence"></param>
        /// <param name="hate"></param>
        /// <param name="selfHarm"></param>
        /// <param name="profanity"></param>
        /// <param name="customBlocklists"></param>
        /// <param name="jailbreak"></param>
        /// <returns> A new <see cref="OpenAI.AzureContentFilterImagePromptResults"/> instance for mocking. </returns>
        public static AzureContentFilterImagePromptResults AzureContentFilterImagePromptResults(AzureContentFilterSeverityResult sexual = null, AzureContentFilterSeverityResult violence = null, AzureContentFilterSeverityResult hate = null, AzureContentFilterSeverityResult selfHarm = null, AzureContentFilterDetectionResult profanity = null, AzureContentFilterBlocklistResult customBlocklists = null, AzureContentFilterDetectionResult jailbreak = null)
        {
            return new AzureContentFilterImagePromptResults(
                sexual,
                violence,
                hate,
                selfHarm,
                serializedAdditionalRawData: null,
                profanity,
                customBlocklists,
                jailbreak);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureOpenAIChatError"/>. </summary>
        /// <param name="code"></param>
        /// <param name="message"></param>
        /// <param name="param"></param>
        /// <param name="type"></param>
        /// <param name="innerError"></param>
        /// <returns> A new <see cref="OpenAI.AzureOpenAIChatError"/> instance for mocking. </returns>
        public static AzureOpenAIChatError AzureOpenAIChatError(string code = null, string message = null, string param = null, string type = null, AzureOpenAIChatErrorInnerError innerError = null)
        {
            return new AzureOpenAIChatError(
                code,
                message,
                param,
                type,
                innerError,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureOpenAIChatErrorInnerError"/>. </summary>
        /// <param name="code"></param>
        /// <param name="revisedPrompt"></param>
        /// <param name="contentFilterResults"></param>
        /// <returns> A new <see cref="OpenAI.AzureOpenAIChatErrorInnerError"/> instance for mocking. </returns>
        public static AzureOpenAIChatErrorInnerError AzureOpenAIChatErrorInnerError(string code = null, string revisedPrompt = null, AzureContentFilterResultForPrompt contentFilterResults = null)
        {
            return new AzureOpenAIChatErrorInnerError(code, revisedPrompt, contentFilterResults, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureOpenAIDalleError"/>. </summary>
        /// <param name="code"></param>
        /// <param name="message"></param>
        /// <param name="param"></param>
        /// <param name="type"></param>
        /// <param name="innerError"></param>
        /// <returns> A new <see cref="OpenAI.AzureOpenAIDalleError"/> instance for mocking. </returns>
        public static AzureOpenAIDalleError AzureOpenAIDalleError(string code = null, string message = null, string param = null, string type = null, AzureOpenAIDalleErrorInnerError innerError = null)
        {
            return new AzureOpenAIDalleError(
                code,
                message,
                param,
                type,
                innerError,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureOpenAIDalleErrorInnerError"/>. </summary>
        /// <param name="code"></param>
        /// <param name="revisedPrompt"></param>
        /// <param name="contentFilterResults"></param>
        /// <returns> A new <see cref="OpenAI.AzureOpenAIDalleErrorInnerError"/> instance for mocking. </returns>
        public static AzureOpenAIDalleErrorInnerError AzureOpenAIDalleErrorInnerError(string code = null, string revisedPrompt = null, AzureContentFilterImagePromptResults contentFilterResults = null)
        {
            return new AzureOpenAIDalleErrorInnerError(code, revisedPrompt, contentFilterResults, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureImage"/>. </summary>
        /// <param name="b64Json"> The base64-encoded JSON of the generated image, if `response_format` is `b64_json`. </param>
        /// <param name="url"> The URL of the generated image, if `response_format` is `url` (default). </param>
        /// <param name="revisedPrompt"> The prompt that was used to generate the image, if there was any revision to the prompt. </param>
        /// <param name="promptFilterResults"></param>
        /// <param name="contentFilterResults"></param>
        /// <returns> A new <see cref="OpenAI.AzureImage"/> instance for mocking. </returns>
        public static AzureImage AzureImage(BinaryData b64Json = null, Uri url = null, string revisedPrompt = null, AzureContentFilterImagePromptResults promptFilterResults = null, AzureContentFilterImageResponseResults contentFilterResults = null)
        {
            return new AzureImage(
                b64Json,
                url,
                revisedPrompt,
                serializedAdditionalRawData: null,
                promptFilterResults,
                contentFilterResults);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureOpenAIErrorResponseAzureOpenAiChatError"/>. </summary>
        /// <param name="error"></param>
        /// <returns> A new <see cref="OpenAI.AzureOpenAIErrorResponseAzureOpenAiChatError"/> instance for mocking. </returns>
        public static AzureOpenAIErrorResponseAzureOpenAiChatError AzureOpenAIErrorResponseAzureOpenAiChatError(AzureOpenAIChatError error = null)
        {
            return new AzureOpenAIErrorResponseAzureOpenAiChatError(error, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="OpenAI.AzureOpenAIErrorResponseAzureOpenAiDalleError"/>. </summary>
        /// <param name="error"></param>
        /// <returns> A new <see cref="OpenAI.AzureOpenAIErrorResponseAzureOpenAiDalleError"/> instance for mocking. </returns>
        public static AzureOpenAIErrorResponseAzureOpenAiDalleError AzureOpenAIErrorResponseAzureOpenAiDalleError(AzureOpenAIDalleError error = null)
        {
            return new AzureOpenAIErrorResponseAzureOpenAiDalleError(error, serializedAdditionalRawData: null);
        }
    }
}
